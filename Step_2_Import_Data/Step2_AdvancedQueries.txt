Search Query 1: search=*&$filter=rating_count eq 550
Result for Query 1: { "@odata.context": "https://enriched-course-catalog-search.search.windows.net/indexes('courses-index')/$metadata#docs(*)", "value": [ { "@search.score": 1, "Key": "company-moodle30e3c6e5-9415-4d85-8229-c2133203c535", "description": "Learn the policies related to the distribution and use of computers, phones, software, and other technology", "duration": 1, "instructor": "Mike Montoya", "level": "beginner", "product": "NA", "rating_average": 4.9, "rating_count": 550, "role": "all", "source": "Company Moodle", "title": "Onboarding - Technology Policies ", "url": "https://www.example.com/course2", "keyphrases": [ "other technology", "policies", "distribution", "use", "computers", "phones", "software" ], "instructor_data": null, "entities": null } ] }

Search Query 2 : search=*&facet=role
Result of Query 2 : { "@odata.context": "https://enriched-course-catalog-search.search.windows.net/indexes('courses-index')/$metadata#docs(*)", "@search.facets": { "role": [ { "count": 6, "value": "developer" }, { "count": 5, "value": "all" }, { "count": 2, "value": "admin" }, { "count": 2, "value": "architect" }, { "count": 2, "value": "business-user" }, { "count": 1, "value": "ai-engineer" }, { "count": 1, "value": "functional-consultant" }, { "count": 1, "value": "solution-architect" } ] }, "value": [ { "@search.score": 1, "Key": "company-moodle30e3c6e5-9415-4d85-8229-c2133203c535", "description": "Learn the policies related to the distribution and use of computers, phones, software, and other technology", "duration": 1, "instructor": "Mike Montoya", "level": "beginner", "product": "NA", "rating_average": 4.9, "rating_count": 550, "role": "all", "source": "Company Moodle", "title": "Onboarding - Technology Policies ", "url": "https://www.example.com/course2", "keyphrases": [ "other technology", "policies", "distribution", "use", "computers", "phones", "software" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodle85ee725b-4ae0-4719-8785-ddf99e19faf1", "description": "For administrators, learn our best practices for securing all databases", "duration": 3, "instructor": "Eileen Diaz", "level": "advanced", "product": "SQL", "rating_average": 4.3, "rating_count": 45, "role": "admin", "source": "Company Moodle", "title": "Security for database admins", "url": "https://www.example.com/course8", "keyphrases": [ "best practices", "administrators", "databases" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodle9700e1dc-b293-4306-9e1b-0d345863db54", "description": "This course will teach you the specific ways our company uses Git. You will learn details for comments, branching, pull requests, and other procsses", "duration": 3, "instructor": "Claudia Blackman", "level": "beginner", "product": "git", "rating_average": 4.5, "rating_count": 125, "role": "developer", "source": "Company Moodle", "title": "Git Workflow ", "url": "https://www.example.com/course3", "keyphrases": [ "specific ways", "other procsses", "course", "company", "Git", "details", "comments", "requests" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodled3f0c955-ac6e-4ced-b91b-ffcef3e8cede", "description": "For developers, learn our best practices for writing secure code for web, server, and desktop development", "duration": 3, "instructor": "Eileen Diaz", "level": "intermediate", "product": "NA", "rating_average": 4.4, "rating_count": 132, "role": "developer", "source": "Company Moodle", "title": "Code security", "url": "https://www.example.com/course9", "keyphrases": [ "best practices", "secure code", "desktop development", "developers", "web", "server" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodle578a3319-aa7c-4d2f-b6a4-39e9638b0a85", "description": "For administrators, this course will teach you how our CI/CD pipelines work from an operations perspective", "duration": 5, "instructor": "Claudia Blackman", "level": "intermediate", "product": "jenkins", "rating_average": 4.9, "rating_count": 56, "role": "admin", "source": "Company Moodle", "title": "DevOps for Ops", "url": "https://www.example.com/course5", "keyphrases": [ "CI/CD pipelines", "operations perspective", "administrators", "course" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodlec014cf36-2a83-4ded-bfe5-d62f405ba970", "description": "This course will teach you best practices for communicating with your team while working remotely", "duration": 1, "instructor": "Gerald Dominguez", "level": "beginner", "product": "NA", "rating_average": 4.7, "rating_count": 325, "role": "all", "source": "Company Moodle", "title": "Remote work", "url": "https://www.example.com/course11", "keyphrases": [ "best practices", "course", "team" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodlec6b7fbd0-7390-4370-ab77-45027596b520", "description": "Understand ways you can be more healthy in the work environment including what ergonomic equipment is available to you", "duration": 1, "instructor": "Mike Montoya", "level": "beginner", "product": "NA", "rating_average": 4.6, "rating_count": 525, "role": "all", "source": "Company Moodle", "title": "Workplace Health", "url": "https://www.example.com/course13", "keyphrases": [ "work environment", "ergonomic equipment", "ways" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodleb51ede14-025f-49ad-9e9e-44ad284eedda", "description": "For developers, this course will teach you how to hook your dev work into our existing CI/CD pipelines.", "duration": 3, "instructor": "Claudia Blackman", "level": "intermediate", "product": "jenkins", "rating_average": 3.8, "rating_count": 101, "role": "developer", "source": "Company Moodle", "title": "DevOps for Dev", "url": "https://www.example.com/course4", "keyphrases": [ "existing CI/CD pipelines", "dev work", "developers", "course" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "ms-learn015c2947-8e4e-44f5-b342-0564f0ee2fbf", "description": "Learn to manage LUIS apps through versioning, key management, handling data, and improving predictions.", "duration": 24, "instructor": "", "level": "intermediate", "product": "azure", "rating_average": 4.77, "rating_count": 106, "role": "developer", "source": "MS Learn", "title": "Manage your Language Understanding Intelligent Service (LUIS) Apps", "url": "https://docs.microsoft.com/en-us/learn/modules/manage-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi", "keyphrases": [ "LUIS apps", "key management", "versioning", "data", "predictions" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "ms-learn0197c8c6-dfc4-450b-9fa7-3f610977cc79", "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.", "duration": 55, "instructor": "", "level": "beginner", "product": "ai-builder", "rating_average": 4.61, "rating_count": 197, "role": "business-user", "source": "MS Learn", "title": "Get started with AI Builder Text recognition", "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-text-recognition/?WT.mc_id=api_CatalogApi", "keyphrases": [ "AI Builder Text recognition", "other Power Platform products" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "ms-learn000a0d57-a0fe-4386-829c-99074d1b3b9b", "description": "Find out about automated testing that proves your code to be maintainable, understandable, and functioning without repetitive manual testing.", "duration": 82, "instructor": "", "level": "beginner", "product": "azure-devops", "rating_average": 4.73, "rating_count": 3301, "role": "solution-architect", "source": "MS Learn", "title": "Run quality tests in your build pipeline by using Azure Pipelines", "url": "https://docs.microsoft.com/en-us/learn/modules/run-quality-tests-build-pipeline/?WT.mc_id=api_CatalogApi", "keyphrases": [ "repetitive manual testing", "automated testing", "code" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodle6b5d3f55-eb02-499d-9775-2e0e25659e07", "description": "Learn our company's Principles for the Responsible Use of AI", "duration": 1, "instructor": "Eileen Diaz", "level": "intermediate", "product": "NA", "rating_average": 4.3, "rating_count": 24, "role": "architect", "source": "Company Moodle", "title": "Ethics in AI", "url": "https://www.example.com/course12", "keyphrases": [ "Responsible Use", "company", "Principles", "AI" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "ms-learn00baaa75-89fe-4f86-805f-f08336e6af48", "description": "Explore the strategic components, use cases, and special factors of an enterprise AI strategy that creates real business value, with INSEAD and Microsoft.", "duration": 70, "instructor": "", "level": "intermediate", "product": "m365", "rating_average": 4.71, "rating_count": 2779, "role": "business-user", "source": "MS Learn", "title": "Define an AI strategy to create business value", "url": "https://docs.microsoft.com/en-us/learn/modules/ai-strategy-to-create-business-value/?WT.mc_id=api_CatalogApi", "keyphrases": [ "enterprise AI strategy", "real business value", "strategic components", "special factors", "cases", "INSEAD", "Microsoft" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodle5b293283-81e6-4f89-a0ab-7053988d6f6a", "description": "Learn how to track billable and non-billable hours by assigning time to projects and other relevant time codes", "duration": 1, "instructor": "Mike Montoya", "level": "beginner", "product": "NA", "rating_average": 4.8, "rating_count": 540, "role": "all", "source": "Company Moodle", "title": "Onboarding - Time Tracking ", "url": "https://www.example.com/course1", "keyphrases": [ "other relevant time codes", "non-billable hours", "projects" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodle9df844fd-eefe-4880-8341-936732174bb5", "description": "Learn our policies for utilizing encryption including key management for projects", "duration": 3, "instructor": "Eileen Diaz", "level": "advanced", "product": "NA", "rating_average": 4.2, "rating_count": 95, "role": "architect", "source": "Company Moodle", "title": "Encryption and security", "url": "https://www.example.com/course14", "keyphrases": [ "key management", "policies", "encryption", "projects" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "ms-learn002f4436-7360-4daa-a21d-f9dcd3518589", "description": "Enable business users with key AI use cases", "duration": 34, "instructor": "", "level": "beginner", "product": "power-platform", "rating_average": 4.75, "rating_count": 758, "role": "functional-consultant", "source": "MS Learn", "title": "Enable business users with key AI use cases", "url": "https://docs.microsoft.com/en-us/learn/modules/enable-business-users-with-key-ai-uses-cases/?WT.mc_id=api_CatalogApi", "keyphrases": [ "key AI use cases", "business users" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodle17b1eedc-0e96-4e5b-8199-83a484388efe", "description": "Learn our internal best practices for using the O365 suite including email signatures, file storage and other issues", "duration": 2, "instructor": "Gerald Dominguez", "level": "beginner", "product": "O365", "rating_average": 4.6, "rating_count": 510, "role": "all", "source": "Company Moodle", "title": "O365", "url": "https://www.example.com/course10", "keyphrases": [ "internal best practices", "O365 suite", "email signatures", "file storage", "other issues" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodle3cbb3800-2554-4121-bb3a-f365deb0c3b6", "description": "Learn our best practices for various tools such as Leaflet", "duration": 2, "instructor": "Robert Gillis", "level": "intermediate", "product": "leaflet", "rating_average": 3.9, "rating_count": 28, "role": "developer", "source": "Company Moodle", "title": "Maps", "url": "https://www.example.com/course6", "keyphrases": [ "best practices", "various tools", "Leaflet" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "company-moodle278d299e-ef0e-47fb-8e98-5a31a073519c", "description": "For developers, learn our best practices for securely connecting to databases", "duration": 2, "instructor": "Eileen Diaz", "level": "advanced", "product": "SQL", "rating_average": 4.8, "rating_count": 115, "role": "developer", "source": "Company Moodle", "title": "Security for database code", "url": "https://www.example.com/course7", "keyphrases": [ "best practices", "developers", "databases" ], "instructor_data": null, "entities": null }, { "@search.score": 1, "Key": "ms-learn01731c10-20bb-41e7-ba21-8528669dcdc3", "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps", "duration": 18, "instructor": "", "level": "advanced", "product": "azure", "rating_average": 4.75, "rating_count": 137, "role": "ai-engineer", "source": "MS Learn", "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps", "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi", "keyphrases": [ "Language Understanding Intelligent Service", "LUIS) Apps", "containers" ], "instructor_data": null, "entities": null } ] }

Search Query 3 : search=*&$filter=rating_average gt 4.5&$select=description,instructor,url,rating_average
Result of Query 3 : { "@odata.context": "https://enriched-course-catalog-search.search.windows.net/indexes('courses-index')/$metadata#docs(*)", "value": [ { "@search.score": 1, "description": "Learn the policies related to the distribution and use of computers, phones, software, and other technology", "instructor": "Mike Montoya", "rating_average": 4.9, "url": "https://www.example.com/course2" }, { "@search.score": 1, "description": "For administrators, this course will teach you how our CI/CD pipelines work from an operations perspective", "instructor": "Claudia Blackman", "rating_average": 4.9, "url": "https://www.example.com/course5" }, { "@search.score": 1, "description": "This course will teach you best practices for communicating with your team while working remotely", "instructor": "Gerald Dominguez", "rating_average": 4.7, "url": "https://www.example.com/course11" }, { "@search.score": 1, "description": "Understand ways you can be more healthy in the work environment including what ergonomic equipment is available to you", "instructor": "Mike Montoya", "rating_average": 4.6, "url": "https://www.example.com/course13" }, { "@search.score": 1, "description": "Learn to manage LUIS apps through versioning, key management, handling data, and improving predictions.", "instructor": "", "rating_average": 4.77, "url": "https://docs.microsoft.com/en-us/learn/modules/manage-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi" }, { "@search.score": 1, "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.", "instructor": "", "rating_average": 4.61, "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-text-recognition/?WT.mc_id=api_CatalogApi" }, { "@search.score": 1, "description": "Find out about automated testing that proves your code to be maintainable, understandable, and functioning without repetitive manual testing.", "instructor": "", "rating_average": 4.73, "url": "https://docs.microsoft.com/en-us/learn/modules/run-quality-tests-build-pipeline/?WT.mc_id=api_CatalogApi" }, { "@search.score": 1, "description": "Explore the strategic components, use cases, and special factors of an enterprise AI strategy that creates real business value, with INSEAD and Microsoft.", "instructor": "", "rating_average": 4.71, "url": "https://docs.microsoft.com/en-us/learn/modules/ai-strategy-to-create-business-value/?WT.mc_id=api_CatalogApi" }, { "@search.score": 1, "description": "Learn how to track billable and non-billable hours by assigning time to projects and other relevant time codes", "instructor": "Mike Montoya", "rating_average": 4.8, "url": "https://www.example.com/course1" }, { "@search.score": 1, "description": "Enable business users with key AI use cases", "instructor": "", "rating_average": 4.75, "url": "https://docs.microsoft.com/en-us/learn/modules/enable-business-users-with-key-ai-uses-cases/?WT.mc_id=api_CatalogApi" }, { "@search.score": 1, "description": "Learn our internal best practices for using the O365 suite including email signatures, file storage and other issues", "instructor": "Gerald Dominguez", "rating_average": 4.6, "url": "https://www.example.com/course10" }, { "@search.score": 1, "description": "For developers, learn our best practices for securely connecting to databases", "instructor": "Eileen Diaz", "rating_average": 4.8, "url": "https://www.example.com/course7" }, { "@search.score": 1, "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps", "instructor": "", "rating_average": 4.75, "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi" } ] }

Search Query 4 : search="data analytics" -Context&searchMode=all
Results of Query 4 : { "@odata.context": "https://enriched-course-catalog-search.search.windows.net/indexes('papers-index')/$metadata#docs(*)", "value": [ { "@search.score": 2.757094, "content": "\nPrivacy preservation techniques in big \ndata analytics: a survey\nP. Ram Mohan Rao1,4*, S. Murali Krishna2 and A. P. Siva Kumar3\n\nIntroduction\nThere is an exponential growth in volume and variety of data as due to diverse applica-\ntions of computers in all domain areas. The growth has been achieved due to afford-\nable availability of computer technology, storage, and network connectivity. The large \nscale data, which also include person specific private and sensitive data like gender, zip \ncode, disease, caste, shopping cart, religion etc. is being stored in public domain. The \ndata holder can release this data to a third party data analyst to gain deeper insights and \nidentify hidden patterns which are useful in making important decisions that may help \nin improving businesses, provide value added services to customers [1], prediction, fore-\ncasting and recommendation [2]. One of the prominent applications of data analytics is \nrecommendation systems which is widely used by ecommerce sites like Amazon, Flip \nkart for suggesting products to customers based on their buying habits. Face book does \nsuggest friends, places to visit and even movie recommendation based on our interest. \nHowever releasing user activity data may lead inference attacks like identifying gender \nbased on user activity [3]. We have studied a number of privacy preserving techniques \nwhich are being employed to protect against privacy threats. Each of these techniques \nhas their own merits and demerits. This paper explores the merits and demerits of each \n\nAbstract \n\nIncredible amounts of data is being generated by various organizations like hospitals, \nbanks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not \nonly humans but machines also contribute to data in the form of closed circuit televi-\nsion streaming, web site logs, etc. Tons of data is generated every minute by social \nmedia and smart phones. The voluminous data generated from the various sources \ncan be processed and analyzed to support decision making. However data analytics \nis prone to privacy violations. One of the applications of data analytics is recommen-\ndation systems which is widely used by ecommerce sites like Amazon, Flip kart for \nsuggesting products to customers based on their buying habits leading to inference \nattacks. Although data analytics is useful in decision making, it will lead to serious \nprivacy concerns. Hence privacy preserving data analytics became very important. This \npaper examines various privacy threats, privacy preservation techniques and models \nwith their limitations, also proposes a data lake based modernistic privacy preservation \ntechnique to handle privacy preservation in unstructured data.\n\nKeywords: Data, Data analytics, Privacy threats, Privacy preservation\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nRam Mohan Rao et al. J Big Data (2018) 5:33 \nhttps://doi.org/10.1186/s40537-018-0141-8\n\n*Correspondence: \nrammohan04@gmail.com \n1 Department of Computer \nScience and Engineering, \nMLR Institute of Technology, \nHyderabad, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-018-0141-8&domain=pdf\n\n\nPage 2 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nof these techniques and also describes the research challenges in the area of privacy \npreservation. Always there exists a trade off between data utility and privacy. This paper \nalso proposes a data lake based modernistic privacy preservation technique to handle \nprivacy preservation in unstructured data with maximum data utility.\n\nPrivacy threats in data analytics\nPrivacy is the ability of an individual to determine what data can be shared, and employ \naccess control. If the data is in public domain then it is a threat to individual privacy \nas the data is held by data holder. Data holder can be social networking application, \nwebsites, mobile apps, ecommerce site, banks, hospitals etc. It is the responsibility of \nthe data holder to ensure privacy of the users data. Apart from the data held in public \ndomain, knowing or unknowingly users themself contribute to data leakage. For exam-\nple most of the mobile apps, seek access to our contacts, files, camera etc. and without \nreading the privacy statement we agree for all terms and conditions, there by contribut-\ning to data leakage.\n\nHence there is a need to educate the smart phone users regarding privacy and privacy \nthreats. Some of the key privacy threats include (1) Surveillance; (2) Disclosure; (3) Dis-\ncrimination; (4) Personal embracement and abuse.\n\nSurveillance\n\nMany organizations including retail, e-commerce, etc. study their customers buying \nhabits and try to come up with various offers and value added services [4]. Based on the \nopinion data and sentiment analysis, social media sites does provide recommendations \nof the new friends, places to visit, people to follow etc. This is possible only when they \ncontinuously monitor their customer’s transactions. This is a serious privacy threat as no \nindividual accepts surveillance.\n\nDisclosure\n\nConsider a hospital holding patient’s data which include (Zip, gender, age, disease) [5–7]. \nThe data holder has released data to a third party for analysis by anonymizing sensitive \nperson specific data so that the person cannot be identified. The third party data analyst \ncan map this information with the freely available external data sources like census data \nand can identify person suffering with some disorder. This is how private information of \na person can be disclosed which is considered to be a serious privacy breach.\n\nDiscrimination\n\nDiscrimination is the bias or inequality which can happen when some private informa-\ntion of a person is disclosed. For instance, statistical analysis of electoral results proved \nthat people of one community were completely against the party, which formed the gov-\nernment. Now the government can neglect that community or can have bias over them.\n\nPersonal embracement and abuse\n\nWhenever some private information of a person is disclosed, it can even lead to per-\nsonal embracement or abuse. For example, a person was privately undergoing medica-\ntion for some specific problem and was buying some medicines on a regular basis from a \n\n\n\nPage 3 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nmedical shop. As part of their regular business model, the medical shop may send some \nreminder and offers related to these medicines over phone. If any family member has \nnoticed this, it will lead to personal embracement and even abuse [8].\n\nData analytics activity will affect data Privacy. Many countries are enforcing Privacy \npreservation laws. Lack of awareness is also a major reason for privacy attacks. For \nexample many smart phones users are not aware of the information that is stolen from \ntheir phones by many apps. Previous research shows only 17% of smart phone users are \naware of privacy threats [9].\n\nPrivacy preservation methods\nMany Privacy preserving techniques were developed, but most of them are based on \nanonymization of data. The list of privacy preservation techniques is given below.\n\n • K anonymity\n • L diversity\n • T closeness\n • Randomization\n • Data distribution\n • Cryptographic techniques\n • Multidimensional Sensitivity Based Anonymization (MDSBA).\n\nK anonymity [10]\n\nAnonymization is the process of modifying data before it is given for data analytics [11], \nso that de identification is not possible and will lead to K indistinguishable records if \nan attempt is made to de identify by mapping the anonymized data with external data \nsources. K anonymity is prone to two attacks namely homogeneity attack and back \nground knowledge attack. Some of the algorithms applied include, Incognito [12], Mon-\ndrian [13] to ensure Anonymization. K anonymity is applied on the patient data shown \nin Table 1. The table shows data before anonymization.\n\nK anonymity algorithm is applied with k value as 3 to ensure 3 indistinguishable \nrecords when an attempt is made to identify a particular person’s data. K anonymity is \napplied on the two attributes viz. Zip and age shown in Table 1. The result of applying \nanonymization on Zip and age attributes is shown in Table 2.\n\nTable 1 Patient data, before anonymization\n\nSno Zip Age Disease\n\n1 57677 29 Cardiac problem\n\n2 57602 22 Cardiac problem\n\n3 57678 27 Cardiac problem\n\n4 57905 43 Skin allergy\n\n5 57909 52 Cardiac problem\n\n6 57906 47 Cancer\n\n7 57605 30 Cardiac problem\n\n8 57673 36 Cancer\n\n9 57607 32 Cancer\n\n\n\nPage 4 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nThe above technique has used generalization [14] to achieve Anonymization. Suppose \nif we know that John is 27 year old and lives in 57677 zip codes then we can conclude \nJohn to have Cardiac problem even after anonymization as shown in Table  2. This is \ncalled Homogeneity attack. For example if John is 36 year old and it is known that John \ndoes not have cancer, then definitely John must have Cardiac problem. This is called as \nbackground knowledge attack. Achieving K anonymity [15, 16] can be done either by \nusing generalization or suppression. K anonymity can optimized if the minimal gener-\nalization can be done without huge data loss [17]. Identity disclosure is the major pri-\nvacy threat which cannot be guaranteed by K anonymity [18]. Personalized privacy is the \nmost important aspect of individual privacy [19].\n\nL diversity\n\nTo address homogeneity attack, another technique called L diversity has been proposed. \nAs per L diversity there must be L well represented values for the sensitive attribute (dis-\nease) in each equivalence class.\n\nImplementing L diversity is not possible every time because of the variety of data. L \ndiversity is also prone to skewness attack. When overall distribution of data is skewed \ninto few equivalence classes attribute disclosure cannot be ensured. For example if the \nentire records are distributed into only three equivalence classes then semantic close-\nness of these values may lead to attribute disclosure. Also L diversity may lead to simi-\nlarity attack. From Table 3 it can be noticed that if we know that John is 27 year old and \nlives in 57677 zip, then definitely John is under low income group because salaries of all \n\nTable 2 After applying anonymization on Zip and age\n\nSno Zip Age Disease\n\n1 576** 2* Cardiac problem\n\n2 576** 2* Cardiac problem\n\n3 576** 2* Cardiac problem\n\n4 5790* > 40 Skin allergy\n\n5 5790* > 40 Cardiac problem\n\n6 5790* > 40 Cancer\n\n7 576** 3* Cardiac problem\n\n8 576** 3* Cancer\n\n9 576** 3* Cancer\n\nTable 3 L diversity privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 6k Cardiac problem\n\n3 576** 2* 7k Cardiac problem\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 22k Cardiac problem\n\n6 5790* > 40 24k Cancer\n\n\n\nPage 5 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nthree persons in 576** zip is low compare to others in the table. This is called as similar-\nity attack.\n\nT closeness\n\nAnother improvement to L diversity is T closeness measure where an equivalence class \nis considered to have ‘T closeness’ if the distance between the distributions of sensi-\ntive attribute in the class is no more than a threshold and all equivalence classes have T \ncloseness [20]. T closeness can be calculated on every attribute with respect to sensitive \nattribute.\n\nFrom Table 4 it can be observed that if we know John is 27 year old, still it will be dif-\nficult to estimate whether John has Cardiac problem or not and he is under low income \ngroup or not. T closeness may ensure attribute disclosure but implementing T closeness \nmay not give proper distribution of data every time.\n\nRandomization technique\n\nRandomization is the process of adding noise to the data which is generally done by \nprobability distribution [21]. Randomization is applied in surveys, sentiment analy-\nsis etc. Randomization does not need knowledge of other records in the data. It can be \napplied during data collection and pre processing time. There is no anonymization over-\nhead in randomization. However, applying randomization on large datasets is not possi-\nble because of time complexity and data utility which has been proved in our experiment \ndescribed below.\n\nWe have loaded 10k records from an employee database into Hadoop Distributed File \nSystem and processed them by executing a Map Reduce Job. We have experimented to \nclassify the employees based on their salary and age groups. In order apply randomiza-\ntion we added noise in the form of 5k records which are randomly added to make a data-\nbase of 15k records and following observations were made after running Map Reduce \njob.\n\n • More number of Mappers and Reducers were used as data volume increased.\n • Results before and after randomization were significantly different.\n • Some of the records which are outliers remain unaffected with randomization and \n\nare vulnerable to adversary attack.\n • Privacy preservation at the cost of data utility is not appreciated and hence randomi-\n\nzation may not be suitable for privacy preservation especially attribute disclosure.\n\nTable 4 T closeness privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 16k Cancer\n\n3 576** 2* 9k Skin allergy\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 42k Cardiac problem\n\n6 5790* > 40 8k Flu\n\n\n\nPage 6 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nData distribution technique\n\nIn this technique, the data is distributed across many sites. Distribution of the data can \nbe done in two ways:\n\ni. Horizontal distribution of data\nii. Vertical distribution of data\n\nHorizontal distribution When data is distributed across many sites with same attrib-\nutes then the distribution is said to be horizontal distribution which is described in \nFig. 1.\n\nHorizontal distribution of data can be applied only when some aggregate functions or \noperations are to be applied on the data without actually sharing the data. For example, \nif a retail store wants to analyse their sales across various branches, they may employ \nsome analytics which does computations on aggregate data. However, as part of data \nanalysis the data holder may need to share the data with third party analyst which may \nlead to privacy breach. Classification and Clustering algorithms can be applied on dis-\ntributed data but it does not ensure privacy. If the data is distributed across different \nsites which belong to different organizations, then results of aggregate functions may \nhelp one party in detecting the data held with other parties. In such situations we expect \nall participating sites to be honest with each other [21].\n\nVertical distribution of data When Person specific information is distributed across \ndifferent sites under custodian of different organizations, then the distribution is called \nvertical distribution as shown in Fig. 2. For example, in crime investigations, the police \nofficials would like to know details of a particular criminal which include health, profes-\nsion, financial, personal etc. All this information may not be available at one site. Such a \ndistribution is called vertical distribution where each site holds few set of attributes of a \nperson. When some analytics has to be done data has to be pooled in from all these sites \nand there is a vulnerability of privacy breach.\n\nIn order to perform data analytics on vertically distributed data, where the attributes \nare distributed across different sites under custodian of different parties, it is highly \n\nFig. 1 Distribution of sales data across different sites\n\n\n\nPage 7 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\ndifficult to ensure privacy if the datasets are shared. For example, as part of a police \ninvestigation, the investigating officer wants to access some information about the \naccused from his employer, health department, bank to gain more insights about the \ncharacter of the person. In this process some of the personal and sensitive information \nof the accused may be disclosed to investigating officer leading to personal embarrass-\nment or abuse. Anonymization cannot be applied when entire records are not needed \nfor analytics. Distribution of data will not ensure privacy preservation but it closely \noverlaps with cryptographic techniques.\n\nCryptographic techniques\n\nThe data holder may encrypt the data before releasing the same for analytics. But \nencrypting large scale data using conventional encryption techniques is highly difficult \nand must be applied only during data collection time. Differential privacy techniques \nhave already been applied where some aggregate computations on the data are done \nwithout actually sharing the inputs. For example, if x and y are two data items then a \nfunction F(x, y) will be computed to gain some aggregate information from both x and \ny without actually sharing x and y. This can be applied on when x and y are held with \ndifferent parties as in the case of vertical distribution. However, if the data is at single \nlocation under the custodian of a single organization, then differential privacy can-\nnot be employed. Another similar technique called secure multiparty computation has \nbeen used but proved to be inadequate in privacy preservation. Data utility will be less \nif encryption is applied during data analytics. Thus encryption is not only difficult to \nimplement but it reduces the data utility [22].\n\nMultidimensional Sensitivity Based Anonymization (MDSBA)\n\nBottom up Generalization [23] and Top down Generalization [24] are the conventional \nmethods of Anonymization which were applied on well represented structured data \nrecords. However, applying the same on large scale data sets is very difficult leading to \n\nFig. 2 Vertical distribution of person specific data\n\n\n\nPage 8 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nissues of scalability and information loss. Multidimensional Sensitivity Based Anonymi-\nzation is a improved version of Anonymization proved to be more effective than conven-\ntional Anonymization techniques.\n\nMultidimensional Sensitivity Based Anonymization is an improved Anonymization \n[25] technique such that it can be applied on large data sets with reduced loss of informa-\ntion and predefined quasi identifiers. As part of this technique Apache MAP REDUCE \n[26] framework has been used to handle large data sets. In conventional Hadoop Distrib-\nuted Files System, the data will be divided into blocks of either 64 MB or 128 MB each \nand distributed across different nodes without considering the data inside the blocks. \nAs part of Multidimensional Sensitivity Based Anonymization [27] technique the data is \nsplit into different bags based on the probability distribution of the quasi identifiers by \nmaking use of filters in Apache Pig scripting language.\n\nMultidimensional Sensitivity Based Anonymization makes use of bottom up generali-\nzation but on a set of attributes with certain class values where class represents a sensi-\ntive attributes. Data distribution was made effectively when compared to conventional \nmethod of blocks. Data Anonymization was done using four quasi identifiers using \nApache Pig.\n\nSince the data is vertically partitioned into different groups, it can protect from back-\nground knowledge attack if the bag contains only few attributes. This method also \nmakes it difficult to map the data with external sources to disclose any person specific \ninformation.\n\nIn this method, the implementation was done using Apache Pig. Apache Pig is a script-\ning language, hence development effort is less. However, code efficiency of Apache Pig is \nrelatively less when compared to Map Reduce job because ultimately every Apache Pig \nscript has to be converted into a Map Reduce job. Multidimensional Sensitivity Based \nAnonymization [28] is more appropriate for large scale data but only when the data is at \nrest. Multidimensional Sensitivity Based Anonymization cannot be applied for stream-\ning data.\n\nAnalysis\nVarious privacy preservation techniques have been studied with respect to features \nincluding, type of data, data utility, attribute preservation and complexity. The compari-\nson of various privacy preservation techniques is shown in Table 5.\n\nTable 5 Comparison of privacy preservation techniques\n\nFeatures Privacy preservation techniques\n\nAnonymization \ntechniques\n\nCryptographic \ntechniques\n\nData \ndistribution\n\nRandomization MDSBA\n\nSuitability for unstructured data No No No No Yes\n\nAttribute preservation No No No Yes Yes\n\nDamage to data utility No No Yes No Yes\n\nVery complex to apply No Yes Yes Yes Yes\n\nAccuracy of results of data \nanalytics\n\nNo Yes No No No\n\n\n\nPage 9 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nResults and discussions\nAs part of systematic literature review, it has been observed that all existing mecha-\nnisms of privacy preservation are with respect to structured data. More than 80% of data \nbeing generated today is unstructured [29]. As such, there is a need to address following \nchallenges.\n\ni. Develop concrete solution to protect privacy in both structured and unstructured \ndata.\n\nii. Scalable and robust techniques to be developed to handle large scale heterogeneous \ndata sets.\n\niii. Data should be allowed to stay in its native form without need for transformation \nand data analytics can be carried out while ensuring privacy preservation.\n\niv. New techniques apart from Anonymization must be developed to ensure protection \nagainst key privacy threats which include identity disclosure, discrimination, surveil-\nlance etc.\n\nv. Maximizing data utility while ensuring data privacy.\n\nConclusion\nNo concrete solution for unstructured data has been developed yet. Conventional \ndata mining algorithms can be applied for classification and clustering problems but \ncannot be used in privacy preservation especially when dealing with person specific \ninformation. Machine learning and soft computing techniques can be used to develop \nnew and more appropriate solution to privacy problems which include identity dis-\nclosure that can lead to personal embarrassment and abuse.\n\nThere is a strong need for law enforcement by governments of all countries to \nensure individual privacy. European Union [30] is making an attempt to enforce pri-\nvacy preservation law. Apart from technological solutions, there is a strong need to \ncreate awareness among the people regarding privacy hazards to safeguard them-\nselves form privacy breaches. One of the serious privacy threats is smart phone. Lot \nof personal information in the form of contacts, messages, chats and files are being \naccessed by many apps running in our smart phone without our knowledge. Most \nof the time people do not even read the privacy statement before installing any app. \nHence there is a strong need to educate people on the various vulnerabilities which \ncan contribute to leakage of private information.\n\nWe propose a novel privacy preservation model based on Data Lake concept to \nhold variety of data from diverse sources. Data lake is a repository to hold data from \ndiverse sources in their raw format [31, 32]. Data ingestion from variety of sources can \nbe done using Apache Flume and an intelligent algorithm based on machine learning \ncan be applied to identify sensitive attributes dynamically [33, 34]. The algorithm will \nbe trained with existing data sets with known sensitive attributes and rigorous train-\ning of the model will help in predicting the sensitive attributes in a given data set [35]. \nAccuracy of the model can be improved by adding more layers of training leading \nto deep learning techniques [36]. Advanced computing techniques like Apache Spark \ncan be used in implementing privacy preserving algorithms which is a distributed \n\n\n\nPage 10 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nmassive parallel computing with in memory processing to ensure very fast processing \n[37]. The proposed model is shown in Fig. 3.\n\nData analytics is done on the data collected from various sources. If an ecommerce \nsite would like to perform data analytics, they need transactional data, website logs and \ncustomers opinion through social media pages. A Data lake is used to collect data from \ndifferent sources. Apache Flume is used to ingest data from social media sites, website \nlogs into Hadoop Distributed File System(HDFS). Using SQOOP relational data can be \nloaded into HDFS.\n\nIn Data lake the data can remain in its native form which is either structured or \nunstructured. When data has to be processed, it can be transformed into HIVE tables. A \nHadoop map reduce job using machine learning can be executed on the data to classify \nthe sensitive attributes [38]. The data can be vertically distributed to separate the sensi-\ntive attributes from rest of the data and apply tokenization to map the vertically distrib-\nuted data. The data without any sensitive attributes can be published for data analytics.\n\nAbbreviations\nCCTV: closed circuit television; MDSBA: Multidimensional Sensitivity Based Anonymization.\n\nAuthors’ contributions\nPRMR: as part of Ph.D. work I have done my literature survey and submitted my work in the form of a paper. SMK: \nsupported me in compiling the paper. APSK: suggested necessary amendments and helped in revising the paper. All \nauthors read and approved the final manuscript.\n\nAuthor details\n1 Department of Computer Science and Engineering, MLR Institute of Technology, Hyderabad, India. 2 Department \nof Computer Science and Engineering, Sri Venkateswara College of Engineering, Tirupati, Andhra Pradesh, India. \n3 Department of Computer Science and Engineering, JNTU Anantapur, Anantapuramu, Andhra Pradesh, India. 4 JNTU \nAnantapur, Anantapur, Andhra Pradesh, India. \n\nAcknowledgements\nI would like to thank my guides, for supporting my work and for suggesting necessary corrections.\n\nData Lake\n\nSqoop to load data from RDBMS\n\nApache \nFlume \nto load \nsocial \nmedia \ndata\n\nLoad data from\ndifferent sources\nand varie es into\nHive Table for\nprocessing\n\nHadoop\nMap\nReduce\nJob to\nclassify\nsensi ve\ndata\n\nNovel Privacy \nPreserva on \nalgorithm \nbased on \nver cal \ndistribu on and \ntokeniza on\n\nFig. 3 A Novel privacy preservation model based on vertical distribution and tokenization\n\n\n\nPage 11 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nIf any one is interested in our work, we are ready to provide more details of the map reduce job which we have \nexecuted and the data processing techniques applied. However the data is used in our work, is freely available in many \nrepositories.\n\nFunding\nNo Funding.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 21 March 2018 Accepted: 4 September 2018\n\nReferences\n 1. Ducange Pietro, Pecori Riccardo, Mezzina Paolo. A glimpse on big data analytics in the framework of marketing \n\nstrategies. Soft Comput. 2018;22(1):325–42.\n 2. Chauhan Arun, Kummamuru Krishna, Toshniwal Durga. Prediction of places of visit using tweets. Knowl Inf Syst. \n\n2017;50(1):145–66.\n 3. Yang D, Bingqing Q, Cudre-Mauroux P. Privacy-preserving social media data publishing for personalized ranking-\n\nbased recommendation. IEEE Trans Knowl Data Eng. 2018. ISSN (Print):1041-4347, ISSN (Electronic):1558-2191.\n 4. Liu Y et al. A practical privacy-preserving data aggregation (3PDA) scheme for smart grid. IEEE Trans Ind Inf. 2018.\n 5. Duncan GT et al. Disclosure limitation methods and information loss for tabular data. In: Confidentiality, disclosure \n\nand data access: theory and practical applications for statistical agencies. 2001. p. 135–166.\n 6. Duncan GT, Diane L. Disclosure-limited data dissemination. J Am Stat Assoc. 1986;81(393):10–8.\n 7. Lambert Diane. Measures of disclosure risk and harm. J Off Stat. 1993;9(2):313.\n 8. Spiller K, et al. Data privacy: users’ thoughts on quantified self personal data. Self-Tracking. Cham: Palgrave Macmil-\n\nlan; 2018. p. 111–24.\n 9. Hettig M, Kiss E, Kassel J-F, Weber S, Harbach M. Visualizing risk by example: demonstrating threats arising from \n\nandroid apps. In: Smith M, editor. Symposium on usable privacy and security (SOUPS), Newcastle, UK, July 24–26, \n2013.\n\n 10. Bayardo RJ, Agrawal A. Data privacy through optimal k-anonymization. In: Proceedings 21st international confer-\nence on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 11. Iyengar S. Transforming data to satisfy privacy constraints. In: Proceedings of the eighth ACM SIGKDD international \nconference on knowledge discovery and data mining. New York: ACM; 2002.\n\n 12. LeFevre K, DeWitt DJ, Ramakrishnan R. Incognito: efficient full-domain k-anonymity. In: Proceedings of the 2005 \nACM SIGMOD international conference on management of data. New York: ACM; 2005.\n\n 13. LeFevre K, DeWitt DJ, Ramakrishnan R. Mondrian multidimensional k-anonymity. In: Proceedings of the 22nd inter-\nnational conference (ICDE’06) on data engineering, 2006. New York: ACM; 2006.\n\n 14. Samarati, Pierangela, and Latanya Sweeney. In: Protecting privacy when disclosing information: k-anonymity and its \nenforcement through generalization and suppression. Technical report, SRI International, 1998.\n\n 15. Sweeney Latanya. Achieving k-anonymity privacy protection using generalization and suppression. In J Uncertain \nFuzziness Knowl Based Syst. 2002;10(05):571–88.\n\n 16. Sweeney Latanya. k-Anonymity: a model for protecting privacy. Int J Uncertain, Fuzziness Knowl Based Syst. \n2002;10(05):557–70.\n\n 17. Williams R. On the complexity of optimal k-anonymity. In: Proc. 23rd ACM SIGMOD-SIGACT-SIGART symp. principles \nof database systems (PODS). New York: ACM; 2004.\n\n 18. Machanavajjhala A et al. L-diversity: privacy beyond k-anonymity. In: Proceedings of the 22nd international confer-\nence on data engineering (ICDE’06), 2006. Piscataway: IEEE; 2006.\n\n 19. Xiao X, Yufei T. Personalized privacy preservation. In: Proceedings of the 2006 ACM SIGMOD international confer-\nence on Management of data. New York: ACM; 2006.\n\n 20. Rubner Y, Tomasi T, Guibas LJ. The earth mover’s distance as a metric for image retrieval. Int J Comput Vision. \n2000;40(2):99–121.\n\n 21. Aggarwal CC, Philip SY. A general survey of privacy-preserving data mining models and algorithms. Privacy-preserv-\ning data mining. Springer: US; 2008. p. 11–52.\n\n 22. Jiang R, Lu R, Choo KK. Achieving high performance and privacy-preserving query over encrypted multidimensional \nbig metering data. Future Gen Comput Syst. 2018;78:392–401.\n\n 23. Wang K, Yu PS, Chakraborty S. Bottom-up generalization: A data mining solution to privacy protection. In: Fourth \nIEEE international conference on data mining, 2004 (ICDM’04). Piscataway: IEEE; 2004.\n\n 24. Fung BCM, Wang K, Yu PS. Top-down specialization for information and privacy preservation. In: Proceedings 21st \ninternational conference on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 25. Zhang X et al. A MapReduce based approach of scalable multidimensional anonymization for big data privacy \npreservation on cloud. In: Third international conference on cloud and green computing (CGC), 2013. Piscataway: \nIEEE; 2013.\n\n\n\nPage 12 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\n 26. Zhang X, et al. A scalable two-phase top-down specialization approach for data anonymization using mapreduce \non cloud. IEEE Trans Parallel Distrib Syst. 2014;25(2):363–73.\n\n 27. Al-Zobbi M, Shahrestani S, Ruan C. Improving MapReduce privacy by implementing multi-dimensional sensitivity-\nbased anonymization. J Big Data. 2017;4(1):45.\n\n 28. Al-Zobbi M, Shahrestani S, Ruan C. Implementing a framework for big data anonymity and analytics access control. \nIn: Trustcom/BigDataSE/ICESS, 2017 IEEE. Piscataway: IEEE; 2017.\n\n 29. Schneider C. IBM Blogs; 2016. https ://www.ibm.com/blogs /watso n/2016/05/bigge st-data-chall enges -might \n-not-even-know/.\n\n 30. TCS. Emphasizing the need for government regulations on data privacy; 2016. https ://www.tcs.com/conte nt/dam/\ntcs/pdf/techn ologi es/Cyber -Secur ity/Abstr act/Stren gthen ing-Priva cy-Prote ction -with-the-Europ ean-Gener al-Data-\nProte ction -Regul ation .pdf.\n\n 31. He X, et al. Qoe-driven big data architecture for smart city. IEEE Commun Mag. 2018;56(2):88–93.\n 32. Ramakrishnan R et al. Azure data lake store: a hyperscale distributed file service for big data analytics. In: Proceed-\n\nings of the 2017 ACM international conference on management of data. New York: ACM; 2017.\n 33. Beheshti A et al. Coredb: a data lake service. In: Proceedings of the 2017 ACM on conference on information and \n\nknowledge management. New York: ACM; 2017.\n 34. Shang T et al. A DP Canopy K-means algorithm for privacy preservation of Hadoop platform. In: International sym-\n\nposium on cyberspace safety and security. Cham: Springer; 2017.\n 35. Jia Q et al. Preserving model privacy for machine learning in distributed systems. IEEE Trans Parallel Distrib Syst. \n\n2018;29(8).\n 36. Psychoula I et al. A deep learning approach for privacy preservation in assisted living. arXiv preprint arXiv \n\n:1802.09359 . 2018.\n 37. Guller M. Big data analytics with spark: a practitioner’s guide to using spark for large scale data analysis. New York: \n\nApress; 2015.\n 38. Fung BCM, Wang K, Philip SY. Anonymzing classification data for privacy preservation. IEEE Trans Knowl Data Eng. \n\n2007;19(5):711–25.\n\nhttps://www.ibm.com/blogs/watson/2016/05/biggest-data-challenges-might-not-even-know/\nhttps://www.ibm.com/blogs/watson/2016/05/biggest-data-challenges-might-not-even-know/\nhttps://www.tcs.com/content/dam/tcs/pdf/technologies/Cyber-Security/Abstract/Strengthening-Privacy-Protection-with-the-European-General-Data-Protection-Regulation.pdf\nhttps://www.tcs.com/content/dam/tcs/pdf/technologies/Cyber-Security/Abstract/Strengthening-Privacy-Protection-with-the-European-General-Data-Protection-Regulation.pdf\nhttps://www.tcs.com/content/dam/tcs/pdf/technologies/Cyber-Security/Abstract/Strengthening-Privacy-Protection-with-the-European-General-Data-Protection-Regulation.pdf\nhttp://arxiv.org/abs/1802.09359\nhttp://arxiv.org/abs/1802.09359\n\n\tPrivacy preservation techniques in big data analytics: a survey\n\tAbstract \n\tIntroduction\n\tPrivacy threats in data analytics\n\tSurveillance\n\tDisclosure\n\tDiscrimination\n\tPersonal embracement and abuse\n\n\tPrivacy preservation methods\n\tK anonymity [10]\n\tL diversity\n\tT closeness\n\tRandomization technique\n\tData distribution technique\n\tCryptographic techniques\n\tMultidimensional Sensitivity Based Anonymization (MDSBA)\n\n\tAnalysis\n\tResults and discussions\n\tConclusion\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9jb3Vyc2VjYXRhbG9nc3RvcmFnZW5yai5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL3M0MDUzNy0wMTgtMDE0MS04LnBkZg2", "metadata_title": "Privacy preservation techniques in big data analytics: a survey", "keyphrases": [ "Privacy preservation techniques", "big data analytics", "survey" ], "pubication_name": null, "publishers": null, "doi": null, "publication_date": null }, { "@search.score": 2.564268, "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3 and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data (2019) 6:47 \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence: \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\nPage 4 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate [86]\n\nFuzzy-CSar-AFP [150]\n\nWeighted online sequential extreme learning machine with kernels (WOS-ELMK) [22]\n\nConcept-adapting very fast decision tree (CVFDT) [151]\n\n\n\nPage 13 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nMany researchers have looked at the aspect of the real-time analysis of big data \nstreams but not much attention has been directed towards social media stream pre-\nprocessing. For instance, the social media stream is characterized by incomplete, noisy, \nslang, abbreviated words. Also, contextual meaning of social media post is essential for \nimproved event detection, sentiment analysis or any other social media analytics algo-\nrithms in terms of quality and accuracy [36, 39]. There is the need to give more atten-\ntion to the preprocessing stage of social media stream analysis in the face of incomplete, \nnoisy, slang, and abbreviated words that are pertinent to social media streams. These \nchallenges create opportunities application of new semantic technology approaches, \nwhich are more suited to social media streams [40, 41].\n\nResearch Question 3: What do big data streaming tools and technologies have in common \n\nand their differences in terms of concept, purpose, and capabilities?\n\nThe features of various tools and technologies for big data stream were compared in \norder to answer this question. An overview analysis based on 10 dimensions, which are \ndatabase support, execution model, workload, fault-tolerance, latency, throughput, reli-\nability, operating system, implementation languages and application domain or areas is \npresented in Table 9.\n\nFor organisations with existing applications that have support for SQL, MySQL, SQL \nServer, Oracle Database, for instance, may consider choosing big data streaming tools \nand technologies that have support for their existing databases. There are few big data \nstreaming tools and technology that support virtually any data format. An example of \nsuch is Infochimps Cloud.\n\nThe major big data streaming tools and technologies considered are all suitable for \nstreaming execution model, however out of 19 big data tools and technology compared \nand contrasted in this section, only 10.5% is suitable for streaming, batch, and iterative \nprocessing while 47.4% can handle jobs requiring both batch and streaming processing. \nIt is safer for a job to be executed on a single platform which can accommodate all the \ndependencies required in order to avoid interoperability constraints than combining \ntwo or more platforms or frameworks. The best fit with respect to the choice of big data \nstreaming tools and technologies will depend on the state of data to process, infrastruc-\nture preference, business use case, and kind of results interested in.\n\nVirtually all the big data streaming tools and technologies are memory intensive. This \nimplies that the main performance bottleneck at higher load conditions will be due to \nlack of memory [42]. However, research has shown that the benefit of high intensive \nmemory applications outweighs the performance loss due to long memory latency [43].\n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once” “at-least-once” \nand “exactly-once” message delivery mechanisms while others support one or two of the \nthree delivery mechanisms. “At-most-once” is the cheapest with least implementation \noverhead and highest performance because it can be done in a fire-and-forget fashion \nwithout keeping the state in the transport mechanism or at the sending end. “At-least-\nonce” delivery requires multiple attempts in order to counter transport losses which \nmeans keeping the state at the sending end and having an acknowledgement mechanism \nat the receiving end. “Exactly-once” is the most expensive and has consequently worst \n\n\n\nPage 14 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\nCo\nm\n\npa\nri\n\nso\nn \n\nof\n b\n\nig\n d\n\nat\na \n\nst\nre\n\nam\nin\n\ng \nto\n\nol\ns \n\nan\nd \n\nte\nch\n\nno\nlo\n\ngi\nes\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nBl\noc\n\nkM\non\n\nCa\nss\n\nan\ndr\n\na,\n M\n\non\n-\n\ngo\nD\n\nB,\n X\n\nM\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nul\nti-\n\nsl\nic\n\ne \nm\n\nem\n-\n\nor\ny \n\nal\nlo\n\nca\ntio\n\nn \nan\n\nd \nba\n\ntc\nh \n\nal\nlo\n\nca\ntio\n\nns\n\nC\nhe\n\nck\npo\n\nin\nt, \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nC\n\n +\n+\n\n11\n, P\n\nyt\nho\n\nn\nA\n\nno\nm\n\nal\ny \n\nde\nte\n\nct\nio\n\nn,\n \n\nne\ntw\n\nor\nk \n\nop\ntim\n\niz\na-\n\ntio\nn,\n\n m\nul\n\ntim\ned\n\nia\n \n\nco\nnt\n\nen\nt d\n\nel\niv\n\ner\ny,\n\n \nfin\n\nan\nci\n\nal\n m\n\nar\nke\n\nt \nan\n\nal\nys\n\nis\n, w\n\neb\n \n\nan\nal\n\nyt\nic\n\ns\n\nSp\nar\n\nk \nSt\n\nre\nam\n\nin\ng\n\nKa\nfk\n\na,\n H\n\nBa\nse\n\n, \nH\n\niv\ne \n\nFl\num\n\ne,\n \n\nH\nD\n\nF/\nS3\n\n, \nKi\n\nne\nsi\n\ns, \nTC\n\nP \nso\n\nck\net\n\ns, \nTw\n\nit-\nte\n\nr, \nSQ\n\nL\n\nBa\ntc\n\nh,\n It\n\ner\nat\n\niv\ne,\n\n \nSt\n\nre\nam\n\nin\ng\n\nC\nPU\n\n/m\nem\n\nor\ny \n\nin\nte\n\nns\niv\n\ne\nRD\n\nD\n b\n\nas\ned\n\n \nC\n\nhe\nck\n\n-p\noi\n\nnt\n-\n\nin\ng,\n\n p\nar\n\nal\nle\n\nl \nre\n\nco\nve\n\nry\n, \n\nre\npl\n\nic\nat\n\nio\nn\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nSc\n\nal\na,\n\n P\nyt\n\nho\nn,\n\n \nJa\n\nva\n, R\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nst\nre\n\nam\nin\n\ng \nm\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\n, f\nog\n\n c\nom\n\n-\npu\n\ntin\ng,\n\n in\nte\n\nra\nct\n\niv\ne \n\nan\nal\n\nys\nis\n\n, m\nul\n\ntim\ne-\n\ndi\na \n\nan\nal\n\nys\nis\n\n, c\nlu\n\nst\ner\n\n \nan\n\nal\nys\n\nis\n, fi\n\nlte\nrin\n\ng,\n \n\nre\n-p\n\nro\nce\n\nss\nin\n\ng,\n \n\nca\nch\n\ne \nin\n\nva\nlid\n\nat\nio\n\nn\n\nA\npa\n\nch\ne \n\nSt\nor\n\nm\nSp\n\nou\nt, \n\nH\nBa\n\nse\n, \n\nH\niv\n\ne,\n S\n\nQ\nL,\n\n \nCa\n\nss\nan\n\ndr\na,\n\n \nM\n\nem\nca\n\nch\ned\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n, \n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\n, \n\nre\nco\n\nrd\n-le\n\nve\nl \n\nac\nkn\n\now\nle\n\ndg\ne-\n\nm\nen\n\nt, \nst\n\nat\nel\n\nes\ns \n\nm\nan\n\nag\nem\n\nen\nt\n\nVe\nry\n\n lo\nw\n\nLo\nw\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nC\n\nlo\nju\n\nre\n, J\n\nav\na,\n\n S\nca\n\nla\n, \n\nC\nlo\n\nju\nre\n\n, n\non\n\n-J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nIn\nte\n\nrn\net\n\n o\nf t\n\nhi\nng\n\ns, \nst\n\nre\nam\n\nin\ng \n\nm\nac\n\nhi\nne\n\n \nle\n\nar\nni\n\nng\n, m\n\nul\ntim\n\ne-\ndi\n\na \nan\n\nal\nys\n\nis\n\nYa\nho\n\no!\n S\n\n4\nM\n\nyS\nQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nRi\nch\n\n D\nat\n\na \nFo\n\nrm\nat\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nLo\nw\n\nLo\nw\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\n, P\n\nyt\nho\n\nn,\n C\n+\n+\n\n, \nPe\n\nrl\nO\n\nnl\nin\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nm\non\n\nito\nrin\n\ng,\n fr\n\nau\nd \n\nde\nte\n\nct\nio\n\nn,\n fi\n\nna\nnc\n\nia\nl \n\nda\nta\n\n p\nro\n\nce\nss\n\nin\ng,\n\n \nw\n\neb\n p\n\ner\nso\n\nna\nliz\n\na-\ntio\n\nn \nan\n\nd \nse\n\nss\nio\n\nn \nm\n\nod\nel\n\nlin\ng\n\n\n\nPage 15 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nSa\nm\n\nza\nKa\n\nfk\na,\n\n H\nD\n\nFS\n, \n\nKi\nne\n\nsi\ns, \n\nSt\nre\n\nam\n \n\nco\nns\n\num\ner\n\n, K\ney\n\n-\nva\n\nlu\ne \n\nst\nor\n\nes\n\nSt\nre\n\nam\nin\n\ng,\n b\n\nat\nch\n\n \npr\n\noc\nes\n\nsi\nng\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nC\n\nhe\nck\n\npo\nin\n\nt\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n W\nin\n\ndo\nw\n\ns\nJa\n\nva\n, S\n\nca\nla\n\n, J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nFi\nlte\n\nrin\ng,\n\n re\n-p\n\nro\n-\n\nce\nss\n\nin\ng,\n\n c\nac\n\nhe\n \n\nin\nva\n\nlid\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nFl\nin\n\nk\nKa\n\nfk\na,\n\n F\nlu\n\nm\ne,\n\n \nH\n\nD\nF/\n\nS3\n, \n\nKi\nne\n\nsi\ns, \n\nTC\nP \n\nso\nck\n\net\ns, \n\nTw\nit-\n\nte\nr, \n\nCa\nss\n\nan\ndr\n\na,\n \n\nRe\ndi\n\ns, \nM\n\non\n-\n\ngo\nD\n\nB,\n H\n\nBa\nse\n\n, \nSQ\n\nL\n\nSt\nre\n\nam\nin\n\ng,\n \n\nba\ntc\n\nh,\n it\n\ner\nat\n\niv\ne,\n\n \nin\n\nte\nra\n\nct\niv\n\ne\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nSt\n\nre\nam\n\n re\npl\n\nay\n \n\nan\nd \n\nm\nar\n\nke\nr-\n\nch\nec\n\nkp\noi\n\nnt\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nW\n\nin\ndo\n\nw\ns\n\nJa\nva\n\n, S\nca\n\nla\n, P\n\nyt\nho\n\nn\nO\n\npt\nim\n\niz\nat\n\nio\nn \n\nof\n \n\ne-\nco\n\nm\nm\n\ner\nce\n\n \nse\n\nar\nch\n\n re\nsu\n\nlt,\n \n\nne\ntw\n\nor\nk/\n\nse\nns\n\nor\n \n\nm\non\n\nito\nrin\n\ng \nan\n\nd \ner\n\nro\nr d\n\net\nec\n\ntio\nn,\n\n \nET\n\nL \nfo\n\nr b\nus\n\nin\nes\n\ns \nin\n\nte\nlli\n\nge\nnc\n\ne \nin\n\nfra\n-\n\nst\nru\n\nct\nur\n\ne,\n m\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\nA\npa\n\nch\ne \n\nA\nur\n\nor\na\n\nH\n2,\n\n Ja\nva\n\n m\nap\n\ns, \nM\n\nyB\nat\n\nis\n, \n\nM\nyS\n\nQ\nL,\n\n P\nos\n\nt-\ngr\n\neS\nQ\n\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nem\nor\n\ny \nan\n\nd \ndi\n\nsk\n s\n\npa\nce\n\nPe\nrio\n\ndi\nc \n\nre\nco\n\nv-\ner\n\ny \nch\n\nec\nkp\n\noi\nnt\n\n \nan\n\nd \nro\n\nllb\nac\n\nk\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nPy\n\nth\non\n\nM\non\n\nito\nrin\n\ng \nap\n\npl\nic\n\na-\ntio\n\nns\n s\n\nuc\nh \n\nas\n \n\nfin\nan\n\nci\nal\n\n a\nna\n\nly\nsi\n\ns \nan\n\nd \nm\n\nili\nta\n\nry\n a\n\npp\nli-\n\nca\ntio\n\nns\n\nRe\ndi\n\ns\nKe\n\ny-\nva\n\nlu\ne \n\nst\nor\n\nes\n, \n\nra\nbi\n\ntm\nq,\n\n M\non\n\n-\ngo\n\nD\nB\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nbu\nt \n\npe\nrs\n\nis\nte\n\nnt\n o\n\nn-\ndi\n\nsk\n d\n\nat\nab\n\nas\ne\n\nRe\npl\n\nic\na \n\nm\nig\n\nra\n-\n\ntio\nn,\n\n S\nen\n\ntin\nel\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nU\nbu\n\nnt\nu,\n\n L\nin\n\nux\n, \n\nO\nSX\n\nC\n, C\n\n#,\n Ja\n\nva\n, P\n\nH\nP, \n\nPy\nth\n\non\nW\n\neb\n a\n\nna\nly\n\nsi\ns, \n\nca\nch\n\ne,\n \n\nm\nes\n\nsa\nge\n\n q\nue\n\nue\ns\n\nC\n-S\n\nPA\nRQ\n\nL\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nLo\nw\n\n m\nem\n\nor\ny \n\nus\nag\n\ne\nA\n\nda\npt\n\nat\nio\n\nn\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nJe\nna\n\n \nlib\n\nra\nrie\n\ns\nRe\n\nal\n-t\n\nim\ne \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n s\n\nen\nso\n\nr d\nat\n\na,\n \n\nso\nci\n\nal\n s\n\nem\nan\n\ntic\n \n\nda\nta\n\n, u\nrb\n\nan\n c\n\nom\n-\n\npu\ntin\n\ng\n\nSA\nM\n\nO\nA\n\nH\nBa\n\nse\n, H\n\niv\ne,\n\n C\nas\n\n-\nsa\n\nnd\nra\n\nSt\nre\n\nam\nin\n\ng\nLo\n\nw\n m\n\nem\nor\n\ny \nus\n\nag\ne\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nC\n\nla\nss\n\nifi\nca\n\ntio\nn,\n\n c\nlu\n\nst\ner\n\n-\nin\n\ng,\n s\n\npa\nm\n\n d\net\n\nec\n-\n\ntio\nn,\n\n re\ngr\n\nes\nsi\n\non\n, \n\nfre\nqu\n\nen\nt p\n\nat\nte\n\nrn\n \n\nm\nin\n\nin\ng\n\n\n\nPage 16 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nCQ\nEL\n\nS\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\nRe\nal\n\n-t\nim\n\ne \nre\n\nas\non\n\nin\ng \n\nov\ner\n\n s\nen\n\nso\nr d\n\nat\na,\n\n \nso\n\nci\nal\n\n s\nem\n\nan\ntic\n\n \nda\n\nta\n, u\n\nrb\nan\n\n c\nom\n\n-\npu\n\ntin\ng\n\nET\nA\n\nLI\nS\n\nRD\nF\n\nSt\nre\n\nam\nin\n\ng\nBi\n\nna\nriz\n\nat\nio\n\nn\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nLo\n\nw\nCu\n\nm\nul\n\nat\niv\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nLi\nnu\n\nx,\n M\n\nac\nO\n\nS,\n \n\nA\nnd\n\nro\nid\n\nPr\nol\n\nog\n, J\n\nav\na,\n\n C\n, \n\nSP\nA\n\nRQ\nL,\n\n C\n#,\n\n \nET\n\nA\nLI\n\nS \nLa\n\nng\nua\n\nge\n \n\nfo\nr E\n\nve\nnt\n\ns \n(E\n\nLE\n)\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n \n\nst\nre\n\nam\nin\n\ng \nev\n\nen\nts\n\nXS\nEQ\n\nXM\nL\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nw\nith\n\n \nbu\n\nffe\nrin\n\ng\nch\n\nec\nkp\n\noi\nnt\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nXe\nrc\n\nes\nBi\n\nol\nog\n\nic\nal\n\n d\nat\n\na,\n s\n\noc\nia\n\nl \nne\n\ntw\nor\n\nks\n, u\n\nse\nr \n\nbe\nha\n\nvi\nou\n\nr, \nfin\n\nan\nci\n\nal\n \n\nda\nta\n\n a\nna\n\nly\nsi\n\ns, \nfil\n\nte\nrin\n\ng\n\nIB\nM\n\n In\nfo\n\nSp\nhe\n\nre\n \n\nst\nre\n\nam\ns\n\nPi\ng,\n\n H\niv\n\ne,\n Ja\n\nql\n, \n\nH\nBa\n\nse\n F\n\nlu\nm\n\ne,\n \n\nLu\nce\n\nne\n, A\n\nvr\no,\n\n \nZo\n\noK\nee\n\npe\nr, \n\nO\noz\n\nie\n, O\n\nra\ncl\n\ne \nD\n\nat\nab\n\nas\ne,\n\n \nD\n\nB2\n, N\n\net\nez\n\nza\n, \n\nM\nyS\n\nQ\nL,\n\n A\nst\n\ner\n, \n\nIn\nfo\n\nrm\nix\n\n.\n\nSt\nre\n\nam\nin\n\ng\nCa\n\npt\nur\n\ne \nda\n\nta\nba\n\nse\n \n\nw\nor\n\nkl\noa\n\nds\n a\n\nnd\n \n\nre\npl\n\nay\n th\n\nem\n in\n\n \na \n\nte\nst\n\n d\nat\n\nab\nas\n\ne \nen\n\nvi\nro\n\nnm\nen\n\nt\n\nA\nut\n\nom\nat\n\nic\n \n\nre\nco\n\nve\nry\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne,\n \n\nA\nt l\n\nea\nst\n\n \non\n\nce\n, A\n\nt \nm\n\nos\nt o\n\nnc\ne\n\nLi\nnu\n\nx,\n C\n\nen\ntO\n\nS\nC\n\n +\n+\n\nJa\nva\n\nSP\nL\n\nSp\nac\n\ne \nw\n\nea\nth\n\ner\n p\n\nre\n-\n\ndi\nct\n\nio\nn,\n\n p\nhy\n\nsi\nol\n\nog\ni-\n\nca\nl d\n\nat\na \n\nst\nre\n\nam\ns \n\nan\nal\n\nys\nis\n\n, t\nra\n\nffi\nc \n\nm\nan\n\nag\nem\n\nen\nt, \n\nre\nal\n\n-\ntim\n\ne \npr\n\ned\nic\n\ntio\nns\n\n, \nev\n\nen\nt d\n\net\nec\n\ntio\nn,\n\n \nvi\n\nsu\nal\n\nis\nat\n\nio\nn\n\nG\noo\n\ngl\ne \n\nM\nill\n\n-\nW\n\nhe\nel\n\nBi\ngT\n\nab\nle\n\n, S\npa\n\nn-\nne\n\nr\nSt\n\nre\nam\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny \nan\n\nd \nbl\n\noo\nm\n\n fi\nlte\n\nrin\ng\n\nU\nnc\n\noo\nrd\n\nin\nat\n\ned\n \n\npe\nrio\n\ndi\nc,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nup\n\nst\nre\n\nam\n \n\nba\nck\n\nup\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx\n\nVi\nrt\n\nua\nlly\n\n a\nny\n\n \npr\n\nog\nra\n\nm\nm\n\nin\ng \n\nla\nng\n\nua\nge\n\nA\nno\n\nm\nal\n\ny \nde\n\nte\nct\n\nio\nn,\n\n \nhe\n\nal\nth\n\n m\non\n\nito\nrin\n\ng,\n \n\nim\nag\n\ne \npr\n\noc\nes\n\nsi\nng\n\n, \nne\n\ntw\nor\n\nk \nsw\n\nitc\nh \n\nm\nan\n\nag\nem\n\nen\nt\n\n\n\nPage 17 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nIn\nfo\n\nch\nim\n\nps\n \n\ncl\nou\n\nd\nSQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nH\niv\n\ne,\n P\n\nig\n \n\nW\nuk\n\non\ng,\n\n \nH\n\nad\noo\n\np,\n \n\nRD\nBM\n\nS,\n V\n\nirt\nu-\n\nal\nly\n\n a\nny\n\n d\nat\n\na \nfo\n\nrm\nat\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nD\n\nis\nas\n\nte\nr d\n\nis\nco\n\nve\nry\n\n, \nte\n\nxt\n a\n\nna\nly\n\nsi\ns, \n\nco\nm\n\n-\npl\n\nex\n e\n\nve\nnt\n\n p\nro\n\nce\nss\n\n-\nin\n\ng,\n v\n\nis\nua\n\nlis\nat\n\nio\nn\n\nM\nic\n\nro\nso\n\nft\n \n\nSt\nre\n\nam\nIn\n\nsi\ngh\n\nt\nSQ\n\nL \nSe\n\nrv\ner\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns\n\n.N\nET\n\n, C\n#,\n\n L\nIN\n\nQ\n, R\n\nx\nM\n\nan\nuf\n\nac\ntu\n\nrin\ng \n\npr\noc\n\nes\ns \n\nm\non\n\nito\nr-\n\nin\ng \n\nan\nd \n\nco\nnt\n\nro\nl, \n\nfin\nan\n\nci\nal\n\n d\nat\n\na \nan\n\nal\nys\n\nis\n, o\n\npe\nra\n\n-\ntio\n\nn \nan\n\nal\nyt\n\nic\ns, \n\nw\neb\n\n \nan\n\nal\nyt\n\nic\ns, \n\nev\nen\n\nt \npa\n\ntt\ner\n\nn \nde\n\nte\nct\n\nio\nn\n\nTI\nBC\n\nO\n S\n\ntr\nea\n\nm\n-\n\nBa\nse\n\nO\nra\n\ncl\ne \n\nda\nta\n\nba\nse\n\n, \nSQ\n\nL \nSe\n\nrv\ner\n\n, \nIm\n\npa\nla\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nSy\nnc\n\nhr\non\n\niz\nat\n\nio\nn,\n\n \nre\n\npl\nic\n\nat\nio\n\nn,\n \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne/\n\nat\n m\n\nos\nt \n\non\nce\n\n/\nex\n\nac\ntly\n\n o\nnc\n\ne\n\nW\nin\n\ndo\nw\n\ns, \nM\n\nac\nO\n\nS,\n L\n\nin\nux\n\nR,\n Ja\n\nva\nM\n\nis\nsi\n\non\n c\n\nrit\nic\n\nal\n \n\nan\nal\n\nys\nis\n\n, I\noT\n\n a\nna\n\nly\n-\n\nsi\ns, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n \nan\n\nal\nys\n\nis\n, p\n\nre\ndi\n\nct\niv\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nw\nor\n\nkfl\now\n\n \nop\n\ntim\niz\n\nat\nio\n\nn,\n ri\n\nsk\n \n\nav\noi\n\nda\nnc\n\ne\n\nLa\nm\n\nbd\na \n\nA\nrc\n\nhi\n-\n\nte\nct\n\nur\ne\n\nRD\nBM\n\nS,\n C\n\nas\nsa\n\nn-\ndr\n\na,\n K\n\naf\nka\n\n, D\nat\n\na \nW\n\nar\neh\n\nou\nse\n\ns, \nKi\n\nne\nsi\n\ns \nD\n\nat\na \n\nSt\nre\n\nam\n, H\n\nD\nFS\n\n, \nH\n\nBa\nse\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny/\n\ndi\nsk\n\n \nda\n\nta\nba\n\nse\nRe\n\npl\nic\n\nat\nio\n\nn,\n \n\nch\nec\n\nkp\noi\n\nnt\nLo\n\nw\nLo\n\nw\nEx\n\nac\ntly\n\n o\nnc\n\ne\nU\n\nbu\nnt\n\nu,\n W\n\nin\n-\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, C\n#,\n\n P\nyt\n\nho\nn,\n\n \nPi\n\ng \nLa\n\ntin\nIo\n\nT \nan\n\nal\nys\n\nis\n, t\n\nra\nck\n\nin\ng \n\nre\nal\n\n-t\nim\n\ne \nup\n\nda\nte\n\ns, \nfin\n\nan\nci\n\nal\n ri\n\nsk\n m\n\nan\n-\n\nag\nem\n\nen\nt, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n a\nna\n\nly\nsi\n\ns\n\n\n\nPage 18 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nperformance because, in addition to “at-least-once” delivery mechanism, it requires the \nstate to be kept at the receiving end in order to filter duplicate deliveries. In other words, \n“at-most-once” delivery mechanism implies that the message may be lost while “at-\nleast-once” delivery ensures that messages are not lost and “exactly-once” implies that \nmessage can neither be lost nor duplicated. “Exactly-once” is suitable for many critical \nsystems where duplicate messages are unacceptable.\n\nResearch Question 4: What are the limitations and strengths of big data streaming tools \n\nand technologies?\n\nObservations from the literature reveal that specific big data streaming technology may \nnot provide the full set of features that are required. It is rare to find specific big data \ntechnology that combines key features such as scalability, integration, fault-tolerance, \ntimeliness, consistency, heterogeneity and incompleteness management, and load bal-\nancing. For instance, Spark streaming [16] and Sonora [44] are excellent and efficient \nfor checkpointing but the operator space available to user codes are limited. S4 does not \nguarantee 100% fault-tolerant persistent state [45]. Storm does not guarantee the order-\ning of messages due to its “at-least-once” mechanism for record delivery [46, 47]. Strict \ntransaction ordering is required by Trident to operate [48]. While streaming SQL pro-\nvide simple and succinct solutions to many streaming problems, the complex application \nlogic (such as matrix multiplication) and intuitive state abstractions are expressed with \nthe operational flow of an imperative language rather than a declarative language such as \nSQL [49–51].\n\nMoreover, BlockMon uses batches and cache locality optimization techniques for \nmemory allocation efficiency and data speed up access. However, deadlock may occur \nif data streams are enqueued with a higher rate than that of the block consumption [52]. \nApache Samza solves batch latency processing problems but requires an added layer for \nflow control [53]. Flink is suitable for heavy stream processing and batch-oriented tasks \nalthough it has scaling limitations [46]. Redis’ in-memory data store makes it extremely \nfast although this implies that available memory size determines the size of the Redis \ndata store [54]. While C-SPARQL and CQELS are excellent for combining static and \nstreaming data, they are not suitable when scalability is required [55]. SAMOA is suit-\nable for machine learning paradigm as it focuses on speed/real-time analytics, scales \nhorizontally and is loosely coupled with its underlying distributed computation platform \n[56]. With Lambda architecture, a real-time layer can complement the batch processing \none thereby reducing maintenance overhead and risk for errors as a result of duplicate \ncode bases. In addition, Lambda architecture handles reprocessing, which is one of the \nkey challenges in stream processing. Two main problems with Lambda architecture are \ncode maintenance in two complex distributed systems that need to produce the same \nresult and high operational complexity [57, 58].\n\nSummarily, there exists various tools and technologies for implementing big data \nstreams and there seems to be no big data streaming tool and technology that offers all \nthe key features required for now. While each tool and technology may have its strengths \nand weaknesses, the choice depends on the objective of the research and data availa-\nbility. A decision in favour of the wrong technology may result in increased overhead \ncost and time. The decision should take into consideration empirical analysis along with \n\n\n\nPage 19 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nsystem requirements. In addition, research efforts should also be directed to how to \nimprove on existing big data streaming tools and technologies to provide key features \nsuch as scalability, integration, fault-tolerance, timeliness, consistency, heterogeneity \nand incompleteness management, and load balancing.\n\nResearch Question 5: What are the evaluation techniques or benchmarks that are used \n\nfor evaluating big data streaming tools and technologies?\n\nThe diversity of big data poses a challenge when it comes to developing big data bench-\nmarks that will be suitable for all workload cases. One cannot stick to one big data \nbenchmark because it has been observed that using only one benchmark on differ-\nent data sets do not give the same result. This implies that benchmark testing should \nbe application specific. Subsequently, in evaluating big data system, the identification \nof workload for an application domain is a prerequisite [59]. Most of the existing big \ndata benchmarks are designed to evaluate a specific type of systems or architectures. For \ninstance, HiBench [60] is suitable for benchmarking Hadoop, Spark and streaming work-\nloads, GridMix [61] and PigMix [62] are for MapReduce Hadoop systems. BigBench [63, \n64] is suitable for benchmarking Teradata Aster DBMS, MapReduce systems, Redshift \ndatabase, Hive, Spark and Impala. Presently, BigDataBench [65, 66] seems to be the only \nbig data benchmark that can evaluate a hybrid of different big data systems.\n\nSo far, many researchers have evaluated their work by making use of synthetic and \nreal-life data. Standard benchmark dataset for big data streaming analytics has not been \nwidely adopted. However, few of the researchers that used standardized benchmarking \nare briefly discussed below. The work of [67] was tested with two benchmarks; Word \nCount and Grep. The result showed that the proposed algorithm can effectively handle \nunstable input and the delay of the total event can be limited to an expected range.\n\nThe tool developed by [68] was tested on both car dataset and Wikinews5 dataset in \ncomparison with sequential processing. It was discovered that their tool (pipeline imple-\nmentation) performed better and faster.\n\nKrawczyk and Wozniak used several benchmark datasets which include Breast-Wis-\nconsin, Pima, Yeast3, Voting records, CYP2C19 isoform, RBF for estimating weights for \nthe new incoming data stream with their proposed method against other standard meth-\nods. They also analysed time and memory requirements. Experimental investigation \nresult proved that the proposed method can achieve better [69].\n\nA benchmark evaluation using an English movie review dataset collected from Rotten \nTomatoes website (a de facto benchmark for analysing sentiment applications) was con-\nducted by [70], the result showed that sentiment analysis engine (SAE) proposed by the \nauthors outperformed the bag of words approach.\n\nAuthors’ suite of ideas in [71] outperformed state-of-the-art searching technique \ncalled EBSM. The work of [72] used various datasets such as KDD-Cup 99, Forest Cover \ntype, Household power consumption, etc. They compared their algorithm—parallel \nK-means clustering with k-means and k-means++, the result showed that their algo-\nrithm performed better in terms of speed.\n\n5 http://en.wikin ews.org.\n\nhttp://en.wikinews.org\n\n\nPage 20 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nMozafari et al. in [73] benchmarked their system, XSeq against other general-purpose \nXML engines. The system outperformed other complex event processing engines by two \norders of magnitude improvement.\n\nAuthors in [74] evaluated their work in terms of time, accuracy and memory using \nForest cover type, Poker hand, and electricity datasets. They compared their method, \nadaptive windowing based online ensemble (AWOE) with other standard methods such \nas accuracy updated ensemble (AUE), online accuracy updated ensemble (OAUE), accu-\nracy weighted ensemble (AWE), dynamic weighted majority (DWM) and Lev Bagging \n(Lev). Their proposed approach outperformed other methods in three perspectives \nwhich include suitability in terms of different type of drifts, better resolved appropriate \nsize of block, and efficiency.\n\nThe evaluation performed by [75] using FACup and Super Tuesday datasets showed \nthat their method, which is a hybrid of topic extraction methods (i.e. a combination of \nfeature pivot and document pivot) has high efficiency and accuracy with respect to recall \nand precision.\n\nEvaluating the performance of low-rank reconstruction and prediction scheme, spe-\ncifically, singular spectrum matrix completion (SS-MC) proposed by [76], SensorScope \nGrand St-Bernard dataset6 and Intel Berkeley Research Lab dataset7 were used. The \nauthors compared their proposed method with three state-of-the-art methods; KNN-\nimputation, RegEM and ADMM version of MC and discovered that their method \noutperformed the other methods in terms of pure reconstruction as well as in the \ndemanding case of simultaneous recovery and prediction.\n\nThe authors in [77] evaluated their work using World Cup 1998 and CAIDA \nAnonymized Internet Traces 2011 datasets. When their method, ECM-Sketch (a sketch \nsynopsis that allows effective summarization of streaming data over both time-based \nand count-based sliding windows) was compared with three state-of-the-art algorithms \n(Sketch variants); ECM-RW, ECM-DW, and ECM-EH, variants using randomized waves, \ndeterministic waves and exponential histograms respectively, their method reduce \nmemory and computational requirements by at least one order of magnitude with a very \nsmall loss in accuracy.\n\nThe work of [78] centred on benchmarking real-time vehicle data streaming models \nfor a smart city using a simulator that emulates the data produced by a given amount of \nsimultaneous drivers. Experiment with the simulator shows that streaming processing \nengine such as Apache Kafka could serve as a replacement to custom-made streaming \nservers to achieve low latency and higher scalability together with cost reduction.\n\nA benchmark among Kyvos Insight, Impala and Spark conducted by [79] shows that \nKyvos Insight performed analytical queries with much lower latencies when there is a \nlarge number of concurrent users due to pre-aggregation and incremental code building \n[80].\n\nAuthors in [81] proposed that in addition to execution time and resource utilization, \nmicroarchitecture-level and energy consumption are key to fully understanding the \nbehaviour of big data frameworks.\n\n6 http://lcav.epfl.ch.page-86035 -en.html.\n7 http://db.csail .mit.edu/labda ta/labda ta.html.\n\nhttp://lcav.epfl.ch.page-86035-en.html\nhttp://db.csail.mit.edu/labdata/labdata.html\n\n\nPage 21 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nIn addition, to strengthen the confidence of big data research evaluation or result, \napplication of empirical methods (i.e. tested or evaluated concept or technology for \nevidence-based result) should be highly encouraged. The current status of empirical \nresearch in big data stream analysis is still at an infant stage. The maturity of a research \nfield is directly proportional to the number of publications with empirical result [20, 21]. \nAccording to [21] that conducted a systematic literature mapping to verify the current \nstatus of empirical research in big data, it was found out that only 151 out of 1778 stud-\nies contained empirical result. As a result, more research efforts should be directed to \nempirical research in order to raise the level of confidence of big data research outputs \nthan it is at present.\n\nMoreover, only a few big data benchmarks are suitable for different workloads at pre-\nsent. Research efforts should be geared towards advancing benchmarks that are suitable \nfor evaluating different big data systems. This would go a long way to reduce cost and \ninteroperability issue.\n\nDiscussion\nFrom the analysis, it was observed that there has been a wave of interest in big data \nstream analysis since 2013. The number of papers produced in 2012 was doubled in \n2013. In the same vein, more than double of the papers in 2013 were produced in 2014. \nThere was a relative surge in 2017 having a total of 98 paper while the year 2018 received \n156 papers (see Tables 9, 10 and Fig. 2). The percentage of papers analyzed from journals \nwas 50%; that of conferences was 41% while that of workshop/technical/symposium was \n9% as depicted in Fig. 3. Figure 4 presented the frequency of research efforts from differ-\nent geographical locations with researchers from China taking the lead. \n\nThe selection of big data streaming tools and technologies should be based on the \nimportance of each of the factors such as the shape of the data, data access, availabil-\nity and consistent requirements, workload profile required, and latency requirement. \nCareful selection with respect to open source technology must be made especially when \nchoosing a recent technology still in production. Moreover, the problem to address, the \nunderstanding of the true costs, and benefits of both open and proprietary solutions are \nalso vital when making a selection.\n\nA lot of research efforts have been directed to big data stream analysis but social media \nstream preprocessing is still an open issue. Due to inherent characteristics of social \nmedia stream which include incomplete, noisy, slang, abbreviated words, social media \nstreams present a challenge to big data streams analytics algorithms. There is the need \nto give more attention to the preprocessing stage of social media stream analysis in the \nface of incomplete, noisy, slang, and abbreviated words that are pertinent to social media \nstreams in order to improve big data streams analytics result.\n\nOut of 19 big data streaming tools and technologies compared, 100% support stream-\ning, 47.4% can do both batch and streaming processing while only 10.5% support stream-\ning, batch and iterative processing. Depending on the state of the data to be processed, \ninfrastructure preference, business use case, and kind of results that is of interest, choos-\ning a single big data streaming technology platform that supports all the system require-\nments minimizes the effect of interoperability constraints.\n\n\n\nPage 22 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once”, “at-least-once”, \nand “exactly-once” message delivery mechanisms while others support one or two of \nthe three delivery mechanisms. Having all the three delivery mechanisms give room for \nflexibility.\n\nIt is rare to find a specific big data technology that combines key features such as scal-\nability, integration, fault-tolerance, timeliness, consistency, heterogeneity and incom-\npleteness management, and load balancing. There seems to be no big data streaming \ntool and technology that offers all the key features required for now. This calls for more \nresearch efforts that are directed to building more robust big data streaming tools and \ntechnologies.\n\nFew big data benchmarks are suitable for a hybrid of big data systems at present and \nstandard benchmark datasets for big data streaming analytics have not been widely \nadopted. Hence, research efforts should be geared towards advancing benchmarks that \nare suitable for evaluating different big data systems.\n\nLimitation of the review\nWhile authors explored Scopus, ScienceDirect and EBSCO databases which index high \nimpact journals and conference papers from IEEE, ACM, SpringerLink, and Elsevier to \nidentify all possible relevant articles, it is possible that some other relevant articles from \nother databases such as Web of Science could have been missed.\n\nThe analysis and synthesis are based on interpretation of selected articles by the \nresearch team. The authors attempted to avoid this by cross-checking papers to deal \nwith bias though that cannot completely rule out the possibility of errors. In addition, \nthe authors implemented the inclusion and exclusion criteria in the selection of articles \nand only relevant articles written in the English Language were selected. Building on the \nunderpinning of the findings of the research, while a lot of research has been done with \nrespect to tools and technologies as well as methods and techniques employed in big \ndata streaming analytics, method of evaluation or benchmarks of the technologies of \nvarious workloads for big data streaming analytics have not received much attention. As \nit could be gathered from the literature reviewed that most of the researchers evaluated \ntheir work using either synthetic or real-life datasets.\n\nConclusion and further work\nAs a result of challenges and opportunities presented by the Information Technology \nrevolution, big data streaming analytics has emerged as the new frontier of competition \nand innovation. Organisations who seize the opportunity of big data streaming analytics \nare provided with insights for robust decision making in real-time thereby making them \nto have an edge over their competitors.\n\nIn this paper, the authors have tried to present a holistic view of big data streaming \nanalytics by conducting a comprehensive literature review to understand and identify \nthe tools and technologies, methods and techniques, benchmarks or methods of evalu-\nation employed, and key issues in big data stream analysis to showcase the signpost of \nfuture research directions.\n\n\n\nPage 23 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n10\n\n D\nis\n\ntr\nib\n\nut\nio\n\nn \nof\n\n p\nap\n\ner\ns \n\nov\ner\n\n th\ne \n\nst\nud\n\nie\nd \n\nye\nar\n\ns\n\nYe\nar\n\n20\n04\n\n20\n05\n\n20\n06\n\n20\n07\n\n20\n08\n\n20\n09\n\n20\n10\n\n20\n11\n\n20\n12\n\n20\n13\n\n20\n14\n\n20\n15\n\n20\n16\n\n20\n17\n\n20\n18\n\nTo\nta\n\nl\n\nPa\npe\n\nr\n2\n\n1\n2\n\n3\n5\n\n2\n5\n\n4\n5\n\n10\n22\n\n28\n38\n\n98\n15\n\n6\n38\n\n1\n\n\n\nPage 24 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nAlthough a lot of research efforts have been directed towards big data at rest (i.e. \nbig data batch processing), there has been increased interest in analysing big data \nin mo", "metadata_storage_path": "aHR0cHM6Ly9jb3Vyc2VjYXRhbG9nc3RvcmFnZW5yai5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL3M0MDUzNy0wMTktMDIxMC03LnBkZg2", "metadata_title": "Big data stream analysis: a systematic literature review", "keyphrases": [ "Big data stream analysis", "systematic literature review" ], "pubication_name": null, "publishers": null, "doi": null, "publication_date": null }, { "@search.score": 2.1776235, "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data (2020) 7:13 \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence: \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\nPage 5 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain ( G) to find out significant features are calculated \nas\n\nFeatures are selected based on the  G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4) G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by  G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures,  G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n \n\ni = 1\n\nm = 1\n\n\n\n\n\n \n\nKRi ∗\n\n \n\n x\nj=1 KFj\n\n  \n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ | G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n w \n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data (2020) 7:13 \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope with the limitations of deep learn-\ning matrix factorization integrated with DMRDF can be adapted.\n\nAbbreviations\nDMRDF: Distributed Memory-based Resilient Dataset Filter; FIG: Feature information gain; RDD: Resilient distributed \ndataset; SVM: Support vector machine; LR: Logistic regression; LSA: Latent semantic analysis; PA: Prediction accuracy; \nP@R: Precision; R@R: Recall; MF: Matrix factorization.\n\nAcknowledgements\nNot applicable.\n\nAuthors’ contributions\nSN designed and implemented the model for Pre-launch product prediction. SN analysed and interpreted the customer \nreviews and ratings dataset regarding the pre-launch product prediction. PS supervised the design, implementation \nand analysis of the model for pre-launch product prediction. MC was a major contributor in writing the manuscript. All \nauthors read and approved the final manuscript.\n\nFunding\nNot applicable.\n\nAvailability of data and materials\nThe datasets generated and/or analysed during the current study are available in the Kaggle repository. [snap.stanford.\nedu/data/web-Amazon.html] [40] and [http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts] [39].\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1 Information Technology, School of Engineering, Cochin University of Science & Technology, Kochi 682022, India. \n2 Department of Computer Science, Cochin University of Science & Technology, Kochi 682022, India. 3 Department \nof Ship Technology, Cochin University of Science & Technology, Kochi 682022, India. \n\nReceived: 25 October 2019 Accepted: 17 February 2020\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\n\n\nPage 14 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nReferences\n 1. Lau RY, Liao SY, Kwok RC, Xu K, Xia Y, Li Y. Text mining and probabilistic modeling for online review spam detection. \n\nACM Trans Manag Inform Syst. 2011;2(4):25.\n 2. Lin X, Li Y, Wang X. Social commerce research: definition, research themes and the trends. Int J Inform Manag. \n\n2017;37:190–201.\n 3. Matos CAD, Rossi CAV. Word-of-mouth communications in marketing: a meta-analytic review of the antecedents \n\nand moderators. J Acad Market Sci. 2008;36(4):578–96.\n 4. Jeon S, et al. Redundant data removal technique for efficient big data search processing. Int J Softw Eng Appl. \n\n2013;7.4:427–36.\n 5. Dave K, Lawrence S, and Pennock D. Mining the peanut gallery: opinion extraction and semantic classification of \n\nproduct reviews. WWW’2003.\n 6. Zhou Y, Wilkinson D, Schreiber R, Pan R. Large-scale parallel collaborative filtering for the netflix prize. 2008. p. \n\n337–48. https ://doi.org/10.1007/978-3-540-68880 -8_32.\n 7. Zhang KZK, Benyoucef M. Consumer behavior in social commerce: a literature review. Dec Support Syst. \n\n2016;86:95–108.\n 8. Cui Geng, Lui Hon-Kwong, Guo Xiaoning. The effect of online consumer reviews on new product sales. Int J Electron \n\nComm. 2012;17(1):39–58.\n 9. Manek AS, Shenoy PD, Mohan MC, et al. Detection of fraudulent and malicious websites by analysing user reviews \n\nfor online shopping websites. Int J Knowl Web Intell. 2016;5(3):171–89. https ://doi.org/10.1007/s1128 0-015-0381-x.\n 10. Singh S, and Singh N. Big data analytics. In: Proceedings of the 2012 international conference on communication, \n\ninformation & computing technology (ICCICT), institute of electrical and electronics engineers (IEEE). 2012. p. 1–4. \nhttp://dx.doi.org/10.1109/iccic t.2012.63981 80.\n\n 11. Demchenko Yuri et al. Addressing big data challenges for scientific data infrastructure. In: IEEE 4th Int. conference \ncloud computing technology and science (CloudCom). 2012.\n\n 12. Sihong Xie, Guan Wang, Shuyang Lin and Yu Philip S. Review spam detection via time-series pattern discovery. In: \nACM Proceedings of the 21st international conference companion on World Wide Web. 2012. p. 635–6.\n\n 13. Koren Y, Bell R, Volinsky C. matrix factorization technique for recommender systems. Computer. 2009;8:30–7.\n 14. Salakhutdinov R, Mnih A, & Hinton G. Restricted boltzmann machines for collaborative filtering. In: Proc. of the 24th \n\nInt. conference on machine learning. 2007. p. 791–8.\n 15. Hao MA, King I, Lyu MR. Learning to recommend with explicit and implicit social relations. ACM Trans Intell Syst \n\nTechnol. 2011;2(3):29.\n 16. Bandakkanavar V, Ramesh M, Geeta V. A survey on detection of reviews using sentiment classification of methods. \n\nIJRITCC. 2014;2(2):310–4.\n 17. Gu V, and Li H. Memory or time—performance evaluation for iterative operation on hadoop and spark. In: Proc. of \n\nthe 2013 IEEE 10th Int. Con. on high-performance computing and communications. 2013. https ://doi.org/10.1109/\nhpcc.and.euc.2013.106.\n\n 18. Zhang Hanpeng, Wang Zhaohua, Chen Shengjun, Guo Chengqi. Product recommendation in online social net-\nworking communities—an empirical study of antecedents and a mediator. J Inform Manag. 2019;56(2):185–95.\n\n 19. Ghose A, Ipeirotis PG. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In: \nInt Conference Electron Comm ACM. 2007. p. 303–10.\n\n 20. Chong AY, Ch’ng E, Liu MJ, Li B. Predicting consumer product demands via Big Data: the roles of online promotional \nmarketing and online reviews. Int J Prod Res. 2015;55:1–15. https ://doi.org/10.1080/00207 543.2015.10665 19.\n\n 21. Yang H, Fujimaki R, Kusumura Y, & Liu J. Online Feature Selection. In: Proceedings of the 22nd ACM SIGKDD Int. \nConference on KDD ‘16, 2016. https ://doi.org/10.1145/29396 72.29398 81.\n\n 22. Breese JS, Heckerman D, and Kadie C. Empirical analysis of predictive algorithms for collaborative filtering. In: Proc. \nof the 14th Conf. on Uncertainty in Artifical Intelligence, 1998.\n\n 23. Mukherjee A, Kumar A, Liu B, Wang J, Hsu M, Castellanos M, Ghosh R. Spotting opinion spammers using behavioral \nfootprints. In: Proc. of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining \nChicago, ACM. 2013. p. 632–40.\n\n 24. Makridakis S, Spiliotis E, Assimakopoulos V. Statistical and Machine Learning forecasting methods: concerns and \nways forward. PLoS ONE. 2018;13(3):e0194889. https ://doi.org/10.1371/journ al.pone.01948 89.\n\n 25. Imon A, Roy C, Manos C, Bhattacharjee S. Prediction of rainfall using logistic regression. Pak J Stat Oper Res. 2012. \nhttps ://doi.org/10.18187 /pjsor .v8i3.535.\n\n 26. Chen T, Zhang W, Lu Q, Chen K, Zheng Z, Yu Y. SVD Feature: a toolkit for feature-based collaborative filtering. J Mach \nLearn Res. 2012;13(1):3619–22.\n\n 27. Shi Y, Larson M, Hanjalic A. Collaborative filtering beyond the user-item matrix—a survey of the state of art and \nfuture challenges. ACM Comput Surv. 2014;47(1):3.\n\n 28. Shan H, & Banerjee A. Generalized probabilistic matrix factorizations for collaborative filtering, In Data mining \n(ICDM), IEEE 10th international conference. 2010. p. 1025–30.\n\n 29. Salakhutdinov R, & Mnih A. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In: Proc. of \nthe 25th int. conference on machine learning. 2008. p. 880–7.\n\n 30. Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine \nlearning techniques. J Big Data. 2015;2(1):23.\n\n 31. Wietsma TA, Ricci F. Product reviews in mobile decision aid systems. Francesco: PERMID; 2005. p. 15–8.\n 32. Jianguo C, et al. A disease diagnosis and treatment recommendation system based on big data mining and cloud \n\ncomputing. Inform Sci. 2018;435:124–49.\n 33. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \n\nreviews using Gini-index feature selection method and SVM classifier. World Wide Web. 2017;20:135–54. https ://doi.\norg/10.1007/s1128 0-015-0381-x.\n\n 34. Fan RE, Chang K-W, Hsieh C-J, Wang X-R, Lin C-J. LIBLINEAR: A library for large linear classification. J Mach Learn Res. \n2008;9:1871–4.\n\nhttps://doi.org/10.1007/978-3-540-68880-8_32\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttp://dx.doi.org/10.1109/iccict.2012.6398180\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1080/00207543.2015.1066519\nhttps://doi.org/10.1145/2939672.2939881\nhttps://doi.org/10.1371/journal.pone.0194889\nhttps://doi.org/10.18187/pjsor.v8i3.535\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttps://doi.org/10.1007/s11280-015-0381-x\n\n\nPage 15 of 15Narayanan et al. J Big Data (2020) 7:13 \n\n 35. Ribeiro MT, Singh S, and Guestrin C. Why should I trust you?: Explaining the predictions of any classifier. In: Proc. \nACMSIGKDD Int. Conf. Knowl. Discov. Data Mining. 2016. p. 1135–44.\n\n 36. Luo X, et al. An effective scheme for QoS estimation via alternating direction method-based matrix factorization. \nIEEE Trans Serv Comput. 2019;12(4):503–18.\n\n 37. Liu CL, Hsaio WH, Lee CH, Lu GC and Jou E. Movie rating and review summarization in mobile environment. In: IEEE \ntrans. systems, man and cybernetics, Part C: applications and reviews. 2012. p. 397–407.\n\n 38. Vapnik, VN. The nature of statistical learning theory, Springer, 2nd ed, 1999. Translated by Xu Jianghua, Zhang Xue-\ngong. Beijing: China Machine Press; 2000.\n\n 39. [Dataset] Flipkart-products. http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts.\n 40. [Dataset] https ://snap.stanf ord.edu/data/web-Amazo n.html.\n 41. [Dataset] He R, McAuley J. Ups and downs: modeling the visual evolution of fashion trends with one-class collabora-\n\ntive filtering. WWW; 2016.\n 42. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. 2005; EMNLP.\n 43. Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauley M, Franklin M, Shenker S, Stoica I. Resilient distributed \n\ndatasets: A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82. UC \nBerkeley: EECS Department; 2011.\n\n 44. Davis J, Goadrich M. The relationship between precision-recall and ROC curves, In ICML. 2006. p. 233–40.\n 45. Lee JS, Lee ES. Exploring the usefulness of predicting people’s locations. Procedia Soc Beh Sci. 2014. https ://doi.\n\norg/10.1016/j.sbspr o.2014.04.451.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\nhttps://snap.stanford.edu/data/web-Amazon.html\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\n\n\tImproving prediction with enhanced Distributed Memory-based Resilient Dataset Filter\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethodology\n\tData collection phase\n\tDataset pre-processing\n\tResilient Distributed Dataset\n\n\tPrediction classifiers\n\tLogistic regression (LR)\n\tSupport Vector Machine (SVM)\n\n\tExperimental setup\n\n\tResults and discussions\n\tConclusion and future work\n\tAcknowledgements\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9jb3Vyc2VjYXRhbG9nc3RvcmFnZW5yai5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL3M0MDUzNy0wMjAtMDAyOTIteS5wZGY1", "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter", "keyphrases": [ "Distributed Memory-based Resilient Dataset Filter", "Improving prediction" ], "pubication_name": null, "publishers": null, "doi": null, "publication_date": null } ] }

Search Query 5 : search="Local regression transfer learning"~&queryType=full

Results of Query 5 : { "@odata.context": "https://enriched-course-catalog-search.search.windows.net/indexes('papers-index')/$metadata#docs(*)", "value": [ { "@search.score": 5.015104, "content": "\nLocal regression transfer learning with applications to users’\npsychological characteristics prediction\n\nZengda Guan • Ang Li • Tingshao Zhu\n\nReceived: 3 February 2015 / Accepted: 30 July 2015 / Published online: 14 August 2015\n\n  The Author(s) 2015. This article is published with open access at Springerlink.com\n\nAbstract It is important to acquire web users’ psycho-\n\nlogical characteristics. Recent studies have built computa-\n\ntional models for predicting psychological characteristics\n\nby supervised learning. However, the generalization of\n\nbuilt models might be limited due to the differences in\n\ndistribution between the training and test dataset. To\n\naddress this problem, we propose some local regression\n\ntransfer learning methods. Specifically, k-nearest-neigh-\n\nbour and clustering reweighting methods are developed to\n\nestimate the importance of each training instance, and a\n\nweighted risk regression model is built for prediction.\n\nAdaptive parameter-setting method is also proposed to deal\n\nwith the situation that the test dataset has no labels. We\n\nperformed experiments on prediction of users’ personality\n\nand depression based on users of different genders or dif-\n\nferent districts, and the results demonstrated that the\n\nmethods could improve the generalization capability of\n\nlearning models.\n\nKeywords Local transfer learning   Covariate shift  \nPsychological characteristics prediction\n\n1 Introduction\n\nIn recent decades, people spend more and more time on\n\nInternet, which implies an increasingly important role of\n\nInternet in human lives. To improve online user experience,\n\nonline services should be personalized and tailored to fit\n\nconsumer preference. Psychological characteristics, including\n\nconsistent traits (like personality [1]) and changeable status\n\n(like depression [2, 3]), are considered as key factors in\n\ndetermining personal preference. Therefore, it is critical to\n\nunderstand web user’s personal psychological characteristics.\n\nPersonal psychological characteristics can be reflected\n\nby behaviours. As one type of human behaviour, web\n\nbehaviour is also associated with individual psychological\n\ncharacteristics [4]. With the help of information technol-\n\nogy, web behaviours can be collected and analysed auto-\n\nmatically and timely, which motivates us to identify web\n\nuser’s psychological characteristics through web beha-\n\nviours. Many studies have confirmed that it is possible to\n\nbuild computational models for predicting psychological\n\ncharacteristics based on web behaviours [5, 6].\n\nMost studies build computational models by supervised\n\nlearning, which learns computational models on labelled\n\ntraining dataset and then applies the models on another\n\nindependent test dataset. Supervised learning assumes that\n\nthe distribution of the training dataset should be identical to\n\nthat of test dataset. However, the assumption might not be\n\nsatisfied in many cases, e.g. demographic variation (e.g.\n\nZ. Guan\n\nBusiness School, Shandong Jianzhu University, Jinan, China\n\ne-mail: guanzengda@sdjzu.edu.cn\n\nA. Li\n\nDepartment of Psychology, Beijing Forestry University, Beijing,\n\nChina\n\nA. Li\n\nBlack Dog Institute, University of New South Wales, Sydney,\n\nAustralia\n\ne-mail: ang.li@blackdog.org.au\n\nT. Zhu (&)\n\nInstitute of Psychology, Chinese Academy of Sciences, Beijing,\n\nChina\n\ne-mail: tszhu@psych.ac.cn\n\nT. Zhu\n\nInstitute of Computing Technology, Chinese Academy of\n\nSciences, Beijing, China\n\n123\n\nBrain Informatics (2015) 2:145–153\n\nDOI 10.1007/s40708-015-0017-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\n\n\nvariation of gender and district), which results in the low\n\nperformance of trained models. Previous studies have paid\n\nlittle attention to this problem. In this paper, we build\n\nmodels based on an innovative approach, which do not\n\nneed to make the assumption of identical distribution.\n\nTransfer learning, or known as covariate shift, is intro-\n\nduced and investigated for this purpose.\n\nMost existing covariate shift methods compute the\n\nresampling weight of training dataset and then train a\n\nweighted risk model to predict on test dataset. Commonly,\n\nthese researches use the entire dataset to reweight in the\n\nwhole procedure. We notice that probability density of data\n\npoints is similar to each other in their local neighbour\n\nregion, and this motivates us to use only the local region\n\ninstead of the whole dataset to improve prediction accuracy\n\nand save computation cost. Therefore, we bring in some\n\nlocal learning views to improve covariate shift. In addition,\n\nthe situation can be encountered that people do not know\n\nany labels of the test dataset before they decide to predict\n\nthem, so it is difficult to learn the parameters of learning\n\nmodel. To cope with this problem, we propose an adaptive\n\nparameter-setting method which needs no test dataset label.\n\nBesides, we focus on the regression form of local transfer\n\nlearning since psychological characteristics labels are often\n\nused in the form of continual values.\n\nIn this paper, based on our previous work [7], we intend to\n\nwork on more domains of psychological characteristics pre-\n\ndictions and propose some new local regression transfer\n\nlearning methods, including training-test k-NN method and\n\nadaptive k-NN methods, which are more effective and can\n\nadaptively set the unknown parameter in prediction functions.\n\nThe rest of the paper is organized as follows: we present\n\nthe local regression transfer learning methods in Sect. 2;\n\nwe then introduce the background of covariate shift and\n\nlocal learning, and propose some local transfer learning\n\nmethods to reweight the training dataset and build the\n\nweighted risk regression model. We perform some exper-\n\niments of psychological characteristics prediction and\n\nanalyse the experiment results in Sect. 3. Finally, we\n\nconclude the whole work in the last section.\n\n2 Local regression transfer learning\n\n2.1 Covariate shift\n\nIn this paper, the input dataset is denoted by X and its labels\n\nare denoted by Y. The training dataset is defined as Ztr ¼\nfðxð1Þtr ; y\n\nð1Þ\ntr Þ; :::; ðxðntrÞ\n\ntr ; y\nðntrÞ\ntr Þg   X   Y with a probability\n\ndistribution PtrðX; YÞ, and the test dataset is defined as\n\nZte ¼ fðxð1Þte ; y\nð1Þ\nte Þ; :::; ðxðnteÞ\n\nte ; y\nðnteÞ\nte Þg   X   Y with a proba-\n\nbility distribution PteðX; YÞ.\n\nIt is quite often that the test dataset has a different distri-\n\nbution from the training dataset. We focus on simple covariate\n\nshift that only inputs of the training dataset and inputs of\n\nthe test dataset follow different distributions, i.e. only\n\nPtrðXÞ 6¼ PteðXÞ, while anything else does not change [8].\n\nThen, we will introduce a general solution framework to\n\ncope with covariate shift problems. The key point is to\n\ncompute probability of training data instances within the\n\ntest dataset population, so that people can use labels of the\n\ntraining dataset to learn a test dataset model. We illustrate\n\nthe process as [9, 10] did.\n\nFirstly, we represent the risk function in this situation\n\nand minimize its expected risk:\n\nmin\nh\n\nEðxtr;ytrÞ Pte\nlðxtr; ytr; hÞ ; ð1Þ\n\nwhere lðxtr; ytr; hÞ is the loss function, which depends on an\n\nunknown parameter h, and ðxtr; ytrÞ Pte denotes the\n\nprobability with which ðxtr; ytrÞ belongs to test dataset\n\npopulation.\n\nIt is usually difficult to compute the distribution of Pte, so\n\npeople turn to compute the empirical risk form as follows:\n\nmin\nh\n\nEðx;yÞ Ptr\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ\n\n  min\nh\n\n1\n\nntr\n\nXntr\n\ni¼1\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ:\nð2Þ\n\nIt is usually assumed that PtrðyjxÞ ¼ PteðyjxÞ, i.e. the pre-\n\ndiction functions for both datasets are identical. Then,\nPteðxtr;ytrÞ\nPtrðxtr;ytrÞ is replaced by\n\nPteðxtrÞ\nPtrðxtrÞ. People usually directly com-\n\npute the ratio\nPteðxtrÞ\nPtrðxtrÞ but do not estimate Ptr and Pte inde-\n\npendently, which can avoid generating more errors.\n\nTo estimate the ratio\nPteðxtrÞ\nPtrðxtrÞ , also called the importance,\n\nresearchers construct many kinds of forms of formula 2.\n\nSugiyama et al. [11] computed the importance by mini-\n\nmizing the Kullback–Leibler divergence between training\n\nand test input densities and constructed the prediction\n\nmodel with a series of Gaussian kernel basis functions.\n\nKanamori et al. [12] proposed a method which minimizes\n\nsquares importance biases represented by Gaussian kernel\n\nfunctions centred at test points. Huang et al. [10] used a\n\nkernel mean matching method (KMM) which computed\n\nthe importance by matching test and training distributions\n\nin a reproducing-kernel Hilbert space. Dai et al. [13] and\n\nPardoe et al. [14] proposed a list of boosting-based algo-\n\nrithms for transfer learning.\n\n2.2 Local machine learning\n\nLocal machine learning has shown a comparative advan-\n\ntage in many machine learning tasks [15–17]. In some\n\nsituations, the size of local region of target data imposes a\n\n146 Z. Guan et al.\n\n123\n\n\n\nsignificant effect on prediction accuracy of model [17]. On\n\nthe one hand, too many neighbour points can over-estimate\n\nthe effects of long-distance points which may have little\n\nrelationship with target point. Thus, this may bring\n\nunnecessary interferences to learning process and produce\n\nmore computation cost. In another way, the predicted data\n\npoint can be thought to have similar property only to points\n\nin its small region but not to all points in a very big region.\n\nOn the other hand, too less neighbour points may introduce\n\nstrong noise to local learning.\n\nFor covariate shift, density estimation is important.\n\nThere are many density estimation methods including k-\n\nnearest-neighbour methods, histogram methods and kernel\n\nmethods, which are localized with only a small proportion\n\nof all points which contribute most to the density estima-\n\ntion of a given point [18]. The k-nearest-neighbour\n\napproximation method is represented as follows:\n\nPðxÞ ¼ k\n\nnV\n; ð3Þ\n\nwhere k is the number of nearest neighbours, n is the total\n\nnumber of all data and V is the region volume containing\n\nall nearest neighbours. If the training and test data are in\n\none volume, ratio between densities of both can be repre-\n\nsented as ktr=kte, which do not require to compute nV any\n\nmore. Moreover, Loog [19] proposed a local classification\n\nmethod which estimated the importance by using the\n\nnumber of test data falling in its neighbour region which\n\nconsisted of training and test data. All of these inspired us\n\nto further study local learning within covariate shift.\n\n2.3 Reweighting the importance\n\nA complete covariate shift process is divided into two\n\nstages: reweighting importance of training data, and\n\ntraining a weighted machine learning model for prediction\n\non the test dataset. In the first stage, we reweight the\n\nimportance of training instances by estimating the ratio\n\nPteðxtrÞ=PtrðxtrÞ.\nIn this work, we use local learning to improve the per-\n\nformance in covariate shift. The key point is to use the\n\nneighbourhood of training points to compute their impor-\n\ntance. In fact, this uses the knowledge of density similarity\n\nbetween the training point and its neighbour points.\n\nK-nearest-neighbour and clustering methods are used to\n\ndetermine the neighbourhood of training point and\n\nreweight the importance. Specifically, we first present k-\n\nNN reweighting method, which is simplest and can be seen\n\nas an origin form of all our k-NN methods. Training-test K-\n\nNN reweighting method is an extension of k-NN\n\nreweighting method, and adaptive K-NN reweighting\n\nmethod is an adaptation of training-test K-NN reweighting\n\nmethod to more common situations. Clustering-based\n\nreweighting method is another view about using local\n\nlearning to reweight the importance.\n\n2.3.1 K-NN reweighting method\n\nWe firstly introduce k-nearest-neighbour reweighting\n\nmethods [7], which uses k-nearest test set neighbours of\n\ntraining instance to compute its importance. Gaussian\n\nkernel is chosen to compute density distance between\n\ntraining data and test data. Then the importance can be\n\ncomputed as follows:\n\nWeigðxtrÞ ¼\nXk\n\ni¼1\n\nexp  cjjxtr   x\nðiÞ\nte jj22\n\n   \n; ð4Þ\n\nwhere k represents the number of the nearest test set\n\nneighbours of training data xtr, which determines the size of\n\nthe local region, and c reflects the bandwidth of kernel\n\nfunction and c[ 0. Even though the exponential term in\n\nWeigðxtrÞ decreases according to an exponential law, the\n\nk value is helpful for obtaining an appropriate neighbour\n\nregion and then computing the importance. It is easy to\n\nknow that this k-nearest-neighbour reweighting method can\n\nsave much computation time when the size of dataset is\n\nvery large compared with k.\n\n2.3.2 Training-test K-NN reweighting method\n\nWhen we regard both the training and test neighbours of\n\ngiven training data in a local region, we develop a new k-\n\nnearest-neighbour reweighting method, called training-test\n\nk-NN reweighting method, which uses both training data\n\nand test data. The training-test k-NN reweighting method\n\ntries to use more training data points to balance the effect\n\nwhich is due to that the only training point does not have\n\ncomparable probability with the other test points in the k-\n\nNN reweighting method sometimes, which may reduce the\n\nperformance of the k-NN method. Simply, ktr=kte can be\n\nused as a reweighting formula if the training data and test\n\ndata in the local region are treated to have similar proba-\n\nbility. Further, we put forward the below formula to\n\ncompute the importance after combining Gaussian kernels.\n\nWeigðxtrÞ ¼\n1\nkte\n\nPkte\n\ni¼1 expð cjjxtr   x\nðiÞ\nte jj22Þ\n\n1\nktr\n\nPktr\n\nj¼1 expð cjjxtr   x\nðjÞ\ntr jj22Þ\n\n; ð5Þ\n\nwhere the neighbour region divides into two parts: the\n\ntraining data part with a total number of ktr and the test data\n\npart with a total number of kte. The total number of data in\n\nthe neighbour region is k ¼ ktr þ kte. When we determine\n\nthe k, ktr and kte will be determined automatically. Here,\n\nsince the training point itself is also defined as its neigh-\n\nbour, the denominator cannot be 0.\n\nLocal regression transfer learning with applications to users’ psychological characteristics 147\n\n123\n\n\n\n2.3.3 Adaptive K-NN reweighting method\n\nFor covariate shift methods, how to determine appropriate\n\nparameters is an important issue. Cross validation tech-\n\nnique is used broadly for the problem. However, cross\n\nvalidation technique needs some labelled test data to be as\n\nvalidation dataset. When the prediction model is used in\n\nchanged situation where test data are completely not\n\nlabelled, people cannot apply cross validation. Here, we\n\ngive an empirical parameter estimation way to modify the\n\ntraining-test k-NN reweighting method. We call it adaptive\n\nk-NN reweighting method, which includes how to deter-\n\nmine k and how to determine c.\n\nFor k, we first assign k   n\n3\n8 in the way of Enas and Choi\n\n[20], where n is the population size. Then we reduce k to be\n\na smaller value nneig when Gaussian kernel function ratio\n\ngauðnneig þ 1Þ=gauðnneigÞ is less than a threshold, which\n\nmakes data in the region have similar probability. gau(i) is\n\ndefined as expð cjjxtar   xðiÞjj22Þ. The reason is that, if a too\n\nsmall value gau(i) of nearest-neighbour point i is summed\n\nto compute the density together with other big values, that\n\nwould bring big bias, and thus the point should be gotten\n\nrid of.\n\nAs to the parameter c, we set it as an empirical way\n\nc ¼ 1\n2nneig\n\nPnneig\n\ni¼1 jjxtr   xðiÞjj22Þ. In fact, this way is somehow\n\nlike a way of computing an approximated empirical vari-\n\nance of a dataset.\n\n2.3.4 Clustering-based reweighting method\n\nFinally, we introduce clustering-based reweighting meth-\n\nods [7], which are somehow similar to data-adaptive his-\n\ntogram method [18]. This kind of methods use clustering\n\nalgorithm to generate histograms, whereas it uses training\n\nand test instances in one histogram to estimate the impor-\n\ntance. In detail, clustering is performed on the whole\n\ntraining and test dataset, and PteðxtrÞ=PtrðxtrÞ is estimated\n\nthrough computing the ratio between number of test data\n\nand number of training data in one cluster. The idea is\n\nsimple that training data and test data clustered in one\n\nsmall enough region can be thought to have the equal\n\nprobability and then the importance can be computed with\n\nthe ratio. Thus, we obtain the formula of clustering-based\n\nreweighting method as follows:\n\nWeigðxðiÞtr Þ ¼\njClusteðxðiÞtr Þj\njClustrðxðiÞtr Þj\n\n; ð6Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training data\n\nx\nðiÞ\ntr , and jClustrðxðiÞtr Þj and jClusteðxðiÞtr Þj denote, respectively,\n\nthe number of training data and the number of test data in\n\nthe same cluster which contains x\nðiÞ\ntr .\n\nLike the histogram method, this method may suffer from\n\nhigh-dimensional difficulty. Number of training data and\n\ntest data in their cluster affects the probability estimation,\n\nand it needs very many data in high-dimensional situation.\n\nClustering method also has a big influence on risk of\n\nimportance weighting, because common clustering meth-\n\nods are not accurate density-region division methods.\n\nClustering-based reweighting method can be taken as an\n\napproximate computation way.\n\n2.4 Weighted regression model\n\nWhen we get the importance of all training data in the\n\nprevious stage, we train the weighted learning model and\n\npredict on the test dataset. The importance of training data\n\nis taken as weight of data and is integrated into the fol-\n\nlowing formula:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n   \n  l y\n\nðiÞ\ntr ; f x\n\nðiÞ\ntr\n\n      \n; ð7Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training\n\ninstances x\nðiÞ\ntr and lðyðiÞtr ; f ðx\n\nðiÞ\ntr ÞÞ represents the bias between\n\nthe real value y\nðiÞ\ntr and the prediction value f ðxðiÞtr Þ which is a\n\nregression function. It can be seen that each instance in the\n\nweighted model has a different weight, while the weight in\n\nunweighted models is uniform.\n\nIn this work, we integrate multivariate adaptive regres-\n\nsion splines (MARS) method with local reweighting\n\nmethods. MARS is an adaptive stepwise regression method\n\n[21], and its weighted learning model has the following\n\nform:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n   \n  y\n\nðiÞ\ntr   f x\n\nðiÞ\ntr\n\n      2\n\nf ðxðiÞtr Þ ¼ b0 þ\nXm\n\nj¼1\n\nbjhj x\nðiÞ\ntr\n\n   \n;\n\nð8Þ\n\nwhere hjðxÞ is a constant denoted by C, or a hinge function\n\nwith the form maxð0; x  CÞ or maxð0;C   xÞ, or a product\n\nof two or more hinge functions. m denotes the total steps to\n\nget optimal performance, and f ðxðiÞtr Þ and f ðxðiÞte Þ denote the\n\nprediction values of training data and test data, respec-\n\ntively. This model is trained for solving unknown coeffi-\n\ncients bj.\n\n3 Experiments\n\nOur experiments aim to predict microblog users’ psycho-\n\nlogical characteristics. They include three parts: predicting\n\nusers’ personality across different genders, predicting\n\n148 Z. Guan et al.\n\n123\n\n\n\nusers’ personality across different districts and predicting\n\nusers’ depression across different genders.\n\nIn this paper, personality is evaluated by the Big Five\n\npersonality framework, a wide accepted personality model\n\nin psychology. The Big Five personality model describes\n\nhuman personality with five dimensions as follows:\n\nagreeableness (A), conscientiousness (C), extraversion (E),\n\nneuroticism (N) and openness (O) [22]. Agreeableness\n\nrefers to a tendency to be compassionate and cooperative.\n\nConscientiousness refers to a tendency to be organized and\n\ndependable. Extraversion refers to a tendency to be\n\nsocialized and talkative. Neuroticism refers to a tendency\n\nto experience unpleasant emotions easily. Openness refers\n\nto the degree of intellectual curiosity, creativity and a\n\npreference for novelty. Besides, CES-T scale [23] is\n\nemployed to measure web users’ depression.\n\nWe test the local transfer methods among web users\n\nwith different genders and in different districts. There\n\nexists some relationship between users’ web behaviours\n\nand their personality/depression. Gender is an important\n\nfactor that can effect users’ behaviours, so we choose it as\n\nexample to test the local transfer methods. It is often\n\nencountered that users of the training set and the test set are\n\nin different districts, so we also study the suitability of the\n\nlocal transfer methods in this situation. Depression in male\n\nand female shows difference [24], so we also investigate it.\n\nIn detail, our experiments are to predict male users’ per-\n\nsonality based on female users, predict non-Guangdong\n\nusers’ personality based on Guangdong users and predict\n\nmale users’ depression degree based on female users.\n\n3.1 Experiment setup\n\nIn China, Sina Weibo (weibo.com) is one of the most\n\nfamous microblog service providers and has more than 503\n\nmillion registered users. In this research, we invited Weibo\n\nusers to complete online self-report questionnaire, includ-\n\ning personality and depression scales, and downloaded\n\ntheir digital records of online behaviours with their\n\nconsent.\n\nFor the prediction of personality, between May and\n\nAugust in 2012, we collected data from 562 participants\n\n(male: 215, female: 347; Guangdong: 175, non-Guang-\n\ndong: 387) and extracted 845 features from their online\n\nbehavioural data. The extracted features can be divided\n\ninto five categories: (a) profiles include features like reg-\n\nistration time and demographics (e.g. gender); (b) self-ex-\n\npression behaviours include features reflecting the online\n\nexpression of one’s personal image (e.g. screen name,\n\nfacial picture and self-statement on personal page);\n\n(c) privacy settings include features indicating the concern\n\nabout individual privacy online (e.g. filtering out pri-\n\nvate messages and comments sent by strangers);\n\n(d) interpersonal behaviours include features indicating the\n\noutcomes of social interaction between different users (e.g.\n\nnumber of friends whom a user follows, number of fol-\n\nlowers, categories of friends whom a user follows and\n\ncategories of forwarded microblogs); and (e) dynamic\n\nfeatures can be represented as time series data (e.g.\n\nupdating microblogs in a certain period or using apps in a\n\ncertain period).\n\nFor the prediction of depression, between May and June\n\nin 2013, we collected data from 1000 participants (male:\n\n426, female: 574). Compared with personality experiments,\n\nwe supplemented additional linguistic features in depres-\n\nsion experiments. These linguistic features included the\n\ntotal number of characters, the number of numerals, the\n\nnumber of punctuation marks, the number of personal\n\npronouns, the number of sentiment words, the number of\n\ncognitive words, the number of perceptual processing\n\nwords and so on.\n\nSince all these experiments have very many feature\n\ndimensions and high dimension curse would weaken the\n\nlearning model, we firstly use stepwisefit method in Matlab\n\ntoolbox to reduce dimensions and select the most relevant\n\nfeatures. For the gender-personality experiment, we pro-\n\ncess the female dataset and obtain 25, 14, 19, 25 and 20\n\nfeatures for predicting Big Five dimensions: A, C, E, N and\n\nO, respectively. For the district-personality experiment, the\n\nGuangdong dataset is processed and we obtain 19, 21, 18,\n\n22 and 20 features for A, C, E, N and O, respectively. For\n\nthe depression experiment, the female dataset is processed,\n\nand we obtain 20 features.\n\nIt also must be emphasized that we test whether the\n\ntraining set and the test set follow the same distribution\n\nbefore we do transfer learning. Both T test and Kol-\n\nmogorov–Smirnov test are performed in the two-sample\n\ntest. T test is fit to test dataset with Gaussian distribution,\n\nand Kolmogorov–Smirnov test can test dataset with\n\nunknown distribution. Specifically, we test the datasets\n\nalong each dimension.\n\nIn the experiments, our local transfer learning methods\n\nare compared with non-transfer method, global transfer\n\nmethod and other transfer learning methods. The local\n\ntransfer learning methods include k-NN transfer learning\n\nmethod, training-test k-NN transfer learning method,\n\nadaptive k-NN transfer learning methods and clustering\n\ntransfer learning methods. The non-transfer method does\n\nnot use a transfer learning way and is a traditional method.\n\nThe global transfer method is also a k-NN transfer learning\n\nmethod, but it has a k value equalling the number of all test\n\ndata, i.e. it takes all test data as neighbours. A famous\n\ntransfer learning method called KMM [10] is also used\n\nhere as a baseline method. After reweighting importance,\n\nwe integrate the importance into weighted risk models. We\n\nchoose weighted risk model MARS, which is open source\n\nLocal regression transfer learning with applications to users’ psychological characteristics 149\n\n123\n\n\n\nregression software for Matlab/Octave from (http://www.\n\ncs.rtu.lv/jekabsons/regression.html).\n\nIn all tables and figures of this paper, MARS denotes the\n\nmethod with no transfer learning, KMM denotes combi-\n\nnation of KMM reweighting method and MARS method in\n\na weighted risk form, GkNN denotes global k-NN\n\nreweighting method and MARS, kNN denotes k-NN\n\nreweighting method and MARS, TTkNN denotes training-\n\ntest k-NN reweighting method and MARS, and AkNN1\n\ndenotes adaptive k-NN reweighting method and MARS,\n\nwhere k value is determined as described in Sect. 2.3.3.\n\nAkNN2 denotes completely adaptive k-NN reweighting\n\nmethod and MARS, where k value and c value are both\n\ndetermined as described in Sect. 2.3.3. Clust denotes\n\nclustering-based reweighting method and MARS. KMM,\n\nGkNN, kNN, TTkNN, AkNN1 and Clust all showed the\n\nbest results where their parameter values are assigned the\n\nbest of a series of tried values. In all experiments, we use\n\nmean square error (MSE) for result comparisons.\n\n3.2 Predicting users’ personality across genders\n\nThis task is to predict male users’ personality based on\n\nfemale users’ labelled data and male users’ unlabelled data.\n\nWe firstly perform single-dimension T test and Kol-\n\nmogorov–Smirnov test to test whether male and female\n\ndatasets are drawn from the same distribution. As a result,\n\n3, 1, 2, 3 and 2 features of all 25, 14, 19, 25 and 20 features\n\nare shown to follow different distributions by T test, and 2,\n\n0, 0, 2 and 1 features by Kolmogorov–Smirnov test. All of\n\nthese test results are with probability more than 95 %\n\nconfidence. Thus, it can be thought that there exists some\n\ndistribution divergence between male and female datasets,\n\nthough the divergence is not big. Then, we examine the\n\nperformance of all the local transfer learning methods in\n\nthis experiment.\n\nFrom Table 1, it can be seen that all regression transfer\n\nlearning methods improve much on the prediction accuracy\n\ncompared with non-transfer learning method in all situa-\n\ntions. Local kNN reweighting methods beat global k-NN\n\nreweighting method GkNN in almost all situations. TTkNN\n\nmethod performs better than the others in 3 of 5 personality\n\ndimensions. AkNN1 performs nearly well with other k-NN\n\nreweighting methods, except in the dimension of C.\n\nEspecially, AkNN1 beats GkNN in 4 dimensions, and this\n\nshows the advantage of its fixed k value. For AkNN2, it\n\nperforms better only than MARS method. Clust also shows\n\ncomparable performance compared with other local trans-\n\nfer learning methods.\n\nTo investigate the impact of k value in k-NN\n\nreweighting methods, we take experiment on trait A as an\n\nexample. The results of GkNN, kNN and TTkNN are\n\nshown in Fig. 1. We can see that these methods perform the\n\nbest when the values of k range between 20 and 30. As\n\nk approximates to the total size of test dataset, the perfor-\n\nmances of kNN and TTkNN become equal to GkNN\n\nmethod. For TTkNN method, it performs worse than GkNN\n\nwhen k is 1, and that could be caused by noise. When k of\n\nTTkNN method is very small, i.e. close to 0, outlier point\n\ncan impose a strong influence. When k of TTkNN method is\n\n50, its performance shows an exception and the reason may\n\nbe that the local region caused by k experiences a shake-up.\n\nThus, the value of k can be recognized as a factor affecting\n\nthe prediction performance.\n\nWe then test how prediction accuracy of clustering\n\ntransfer methods is affected by the number of clusters in all\n\nfive personality traits. From Fig. 2, we can see that the\n\nnumber of clusters has a big influence on the prediction\n\naccuracy. There is no certain value of cluster number\n\nwhich achieves the best performance for all five traits. The\n\nmethod obtains the optimization result in C, E and O trait\n\nwhen the number of clusters is small. For these three traits,\n\nTable 1 Local regression transfer learning results for predicting\n\npersonality across different-gender datasets. MSE is used to measure\n\nthe test results\n\nCondition A C E N O\n\nMARS 34.8431 45.9335 34.0655 29.5776 32.6700\n\nKMM 26.7654 30.8683 24.0116 27.9208 28.1425\n\nGkNN 25.2125 31.5119 23.1247 27.6345 30.6127\n\nkNN 24.3776 31.1357 23.1247 27.4160 28.2948\n\nTTkNN 24.3149 31.0282 22.8547 27.8493 28.1424\n\nAkNN1 24.3913 31.2013 24.5649 27.4419 28.2027\n\nAkNN2 29.8956 31.0112 24.0063 27.8779 28.1899\n\nClust 27.3070 30.4555 23.9003 27.7718 28.1425\n\n0 50 100 150 200 250 300\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\nk\n\nM\nS\n\nE\n\nGkNN\nkNN\nTTkNN\n\nFig. 1 The impact of the number of nearest neighbours on the\n\nperformance of k-NN transfer methods in trait A\n\n150 Z. Guan et al.\n\n123\n\nhttp://www.cs.rtu.lv/jekabsons/regression.html\nhttp://www.cs.rtu.lv/jekabsons/regression.html\n\n\nit could also be seen that their MSE gradually increases as\n\nnumber of clusters increases, and the least k value (here,\n\nthe value is 1) may not be the optimised value because of\n\nnoise. Meanwhile, it seems to follow no regular rule for the\n\nother two traits. Thus, we can think that there is no constant\n\noptimal value for cluster number in clustering transfer\n\nmethods for all situations. The reasons are speculated that\n\ndistributions of the datasets are of diversity, and clustering\n\nmethod is not a stable density estimation method here.\n\n3.3 Predicting users’ personality across districts\n\nIn this experiment, we use Weibo data of Guangdong\n\nprovince of China to train the model and predict person-\n\nality of users in the other districts. Firstly, we still apply\n\nstepwisefit method to select 19, 21, 18, 22 and 20 features\n\nfrom a total of 845 features in A, C, E, N and O traits,\n\nrespectively. We then use T test and get 3, 1, 3, 3 and 2\n\nfeatures following different distributions and use Kol-\n\nmogorov–Smirnov test and get 3, 5, 6, 9 and 2 features\n\nfollowing different distributions, both with probability\n\nmore than 95% confidence. Finally, we perform our\n\nregression transfer methods on different-district datasets\n\nand compare all the methods as used in the above different-\n\ngender experiment.\n\nWe analyse performances of all methods. Table 2 shows\n\nthat all local transfer learning methods perform better than\n\nnon-transfer method MARS. GkNN behaves unstably: it\n\nperforms worse than MARS in 2 of all 5 traits, while it\n\nperforms best in O trait. kNN performs no worse than\n\nGkNN in all five traits. TTkNN is still the best method for\n\nmost situations and performs stably. AkNN1 performs\n\nmuch better than MARS, but much worse in O trait than\n\nother local transfer learning methods except AkNN2.\n\nAkNN2 behaves only a little better than MARS in four\n\ntraits and weaker in one trait. Clust also beats MARS\n\nmethod in all situations but behaves not so well in O trait.\n\n3.4 Predicting users’ depression across genders\n\nThis experiment is to predict male users’ depression level\n\nbased on female users’ labelled data. Still, stepwisefit\n\nmethod is performed and 20 features are selected. 3 feature\n\ndimensions in T test and 5 feature dimensions in Kol-\n\nmogorov–Smirnov test are thought as different-distribution\n\nfeature. This suggests that training and test data also follow\n\ndifferent distributions in this experiment.\n\nIn Table 3, the result shows that the transfer learning\n\nmethods perform much better than non-transfer method\n\nMARS. KMM and Clust behave a little better than other\n\ntransfer methods. AkNN1 and AkNN2 perform nearly\n\nequally well to other transfer learning methods.\n\n3.5 Discussion and conclusion\n\nIt can be concluded from the above experiments that all our\n\nlocal transfer learning methods work better than non-\n\ntransfer learning method, because they reduce the predic-\n\ntion bias of model which is trained and tested on different-\n\ndistribution datasets. Our local k-NN family transfer\n\nlearning methods perform better than the global k-NN\n\ntransfer learning method generally, and the reason may be\n\nthat an appropriate k value in k-NN methods could reflect\n\nmore subtle nature in density estimation. All our local\n\ntransfer learning methods show comparable performance\n\nwith KMM method in all situations. TTkNN method\n\nexceeds kNN and obtains the best performance among all\n\nthe methods in half of situations. It could be guessed that\n\nTTkNN uses both test and training data information, while\n\nkNN only uses test data. Clust method performs well in\n\n0 20 40 60 80 100 120 140 160 180 200\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\nNumber of Cluster\n\nM\nS\n\nE\n\nA\nC\nE\nN\nO\n\nFig. 2 The impact of cluster number in clustering regression transfer\n\nlearning in trait A, C, E, N and O\n\nTable 2 Local regression transfer learning results for predicting\n\npersonality across different-district datasets. MSE is used to measure\n\nthe test results\n\nCondition A C E N O\n\nMARS 43.6764 65.0172 44.3688 47.4115 229.8742\n\nKMM 42.1194 48.9055 39.3781 47.4057 59.7330\n\nGkNN 44.7136 45.8609 43.0928 49.2114 43.1696\n\nkNN 43.2840 42.0574 38.8104 42.9135 43.1696\n\nTTkNN 38.6335 40.8370 35.0338 41.8510 45.3623\n\nAkNN1 43.5722 41.3360 39.0398 41.4173 195.6540\n\nAkNN2 41.5186 62.8834 39.3683 52.2294 218.9917\n\nClust 39.1079 42.5235 37.7979 44.6171 113.6659\n\nLocal regression transfer learning with applications to users’ psychological characteristics 151\n\n123\n\n\n\nmost situations, this proves its applicability, and better\n\ndensity clustering methods may further enhance this\n\nmethod.\n\nFinally, we compare the performance of GkNN, AkNN1\n\nand AkNN2; AkNN1 is the best, GkNN is the second and\n\nAkNN2 is the worst of them. AkNN1 performs better than\n\nGkNN in most situations, and this demonstrates that\n\ndetermining k in an AkNN1 way, same as AkNN2, can\n\nwork well generally. We also note that AkNN1 and\n\nAkNN2 behave not well in O trait in Table 2, and it\n\nindicates that k in AkNN1 and AkNN2 is not an optimal\n\nchoice in some situation because of the change of distri-\n\nbution of data set. It is pointed that AkNN2 is inferior to\n\nAkNN1 and GkNN, because it does not choose the optimal\n\nvalue for parameter c in prediction function preliminary.\n\nSince no parameter in AkNN2 needs to be set artificially, it\n\ncould work in the situations where we completely have no\n\nidea about labels of predicted data, which can be of much\n\nsignificance.\n\n4 Conclusions\n\nIn this paper, we propose some local regression transfer\n\nlearning methods and apply them to predict users’ psy-\n\nchological characteristics when the training set and the test\n\nset follow different distributions. We present k-NN\n\nreweighting methods and clustering reweighting method to\n\nestimate the importance of training set in covariate shift\n\nprocess. Specifically, these methods utilize training and test\n\ndata in certain local neighbour region for importance\n\nestimations. We still apply them to psychological charac-\n\nteristics predictions including microblog users’ personality\n\nprediction across different genders and different districts,\n\nand microblog users’ depression prediction across different\n\ngenders. The experiments demonstrate that these methods\n\nimprove the accuracy of prediction models. Specially, the\n\ncomplete adaptive k-NN reweighting method is able to\n\nmake prediction even without knowing any label of test\n\ndata.\n\nAcknowledgments The authors gratefully acknowledge the gener-\n\nous support from National Basic Research Program of China\n\n(2014CB744600), National High-tech R&D Program of China\n\n(2013AA01A606), Strategic Priority Research Program (XDA060\n\n30800) and Key Research Program of CAS(KJZD-EW-L04).\n\nOpen Access This article is distributed under the terms of the\n\nCreative Commons Attribution 4.0 International License (http://crea\n\ntivecommons.org/licenses/by/4.0/), which permits unrestricted use, dis-\n\ntribution, and reproduction in any medium, provided you give\n\nappropriate credit to the original author(s) and the source, provide a link\n\nto the Creative Commons license, and indicate if changes were made.\n\nReferences\n\n1. Burger J (2008) Personality, 7th edn. Thomson Higher Education,\n\nBelmont\n\n2. Kessler RC, Angermeyer M, Anthony JC et al (2007) Lifetime\n\nprevalence and age-of-onset distributions of mental disorders in\n\nthe World Health Organization’s World Mental Health Survey\n\nInitiative. World Psychiatry 6(3):168–176\n\n3. Gurland BJ (1992) The impact of depression on quality of life of\n\nthe elderly. Clin Geriatr Med 8:377–386\n\n4. Amichai-Hamburger Y (2002) Internet and personality. Comput\n\nHuman Behav 18:1–10\n\n5. Li L, Li A, Hao B, Guan Z, Zhu T (2014) Predicting active users\n\npersonality based on micro-blogging behaviors. PLoS One\n\n9(1):e84997\n\n6. Zhang F, Zhu T, Li A, Li Y, Xu X (2011) A survey of web\n\nbehavior and mental health. 3rd International symposium of web\n\nsociety (SWS), Port Elizabeth\n\n7. Guan Z, Nie D, Hao B, Bai S and Zhu T (2014) Local regression\n\ntransfer learning for users’ personality prediction. Active media\n\ntechnology\n\n8. Storkey AJ (2009) When training and test sets are different:\n\ncharacterizing learning transfer. In: Quiñonero-Candela J,\n\nSugiyama M, Schwaighofer A, Lawrence N (eds) Dataset shift in\n\nmachine learning. The MIT Press, Cambridge, pp 3–28\n\n9. Pan S, Yang Q (2010) A survey on transfer learning. IEEE Trans\n\nKnowl Data Eng 22(10):1345–1359\n\n10. Huang J, Smola A, Gretton A, Borgwardt K, Scholkopf B (2007)\n\nCorrecting sample selection bias by unlabeled data. In: Pro-\n\nceedings of 19th annual conference neural information processing\n\nsystems\n\n11. Sugiyama M, Nakajima S, Kashima H, Buenau P, Kawanabe M\n\n(2008) Direct importance estimation with model selection and its\n\napplication to covariate shift adaptation. In: Proceedings of 20th\n\nannual conference neural information processing systems\n\n12. Kanamori T, Hido S, Sugiyama M (2008) Efficient direct density\n\nratio estimation for non-stationarity adaptation and outlier\n\ndetection. Adv Neural Inf Processing Syst 20:809–816\n\n13. Dai W, Yang Q, Xue G, Yu Y (2007) Boosting for transfer\n\nlearning. In: Proceedings of the 24th international conference on\n\nmachine learning\n\n14. Pardoe D, Stone P (2010) Boosting for regression transfer. In:\n\nProceedings of the 27th international conference on machine\n\nlearning\n\n15. Holte RC (1993) Very simple classification rules perform well on\n\nmost commonly used data sets. Mach. Learn. 11:63–90\n\n16. Loader C (1999) Local regression and likelihood. Springer, New\n\nYork\n\nTable 3 Local regression transfer learning results for predicting depression across different-gender dataset\n\nMARS KMM GkNN kNN TTkNN AkNN1 AkNN2 Clust\n\nMSE 126.9868 111.2089 113.5784 113.4111 113.1296 113.3482 113.3624 111.5430\n\n152 Z. Guan et al.\n\n123\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n17. Gupta M, Garcia E, Chin E (2008) Adaptive local linear\n\nregression with application to printer color management. IEEE\n\nTrans Image Processing 17:936–945\n\n18. Webb AR (2002) Statistical pattern recognition, 2nd edn. Wiley,\n\nNew York, pp 81–122\n\n19. Loog M (2012) Nearest neighbor-based importance weighting.\n\nIn: IEEE international workshop on machine learning for signal\n\nprocessing, Santander\n\n20. Enas GG, Choi SC (1986) Choice of the smoothing parameter\n\nand efficiency of k-nearest neighbor classification. Comput Math\n\nAppl 12A(2):235–244\n\n21. Hastie T, Tibshirani R, Friedman J (2008) The elements of sta-\n\ntistical learning, 2nd edn. Springer, New York\n\n22. Funder D (2001) Personality. Annu Rev Psychol 52:197–221\n\n23. Radloff LS (1977) The CES-D scale: a self-report depression\n\nscale for research in the general population. App Psychol Meas\n\n1:385\n\n24. Hankin B, Abramson L, Moffitt T, Silva P, McGee R, Angell K\n\n(1998) Development of depression from preadolescence to young\n\nadulthood: emerging gender differences in a 10-year longitudinal\n\nstudy. J Abnormal Psychol 107(1):128–140\n\nZengda Guan received his Ph.D. degree in computer science and\n\napplications from University of Chinese Academy of Sciences,\n\nBeijing, China, in 2014. He is currently a lecturer at the Department\n\nof E-commerce, Shandong Jianzhu University. His research interests\n\nlie in the area of transfer learning and data mining.\n\nAng Li received his Ph.D. degree at the Chinese Academy of\n\nSciences. He received postdoctoral training at the Black Dog Institute,\n\nUniversity of New South Wales and the NHMRC Centre of Research\n\nExcellence in Suicide Prevention (CRESP). Currently, he is the\n\nlecturer at the Department of Psychology, Beijing Forestry Univer-\n\nsity. His research efforts are focused on early detection and\n\nintervention of mental health problems via the Internet (eHealth).\n\nTingshao Zhu earned his second Ph.D. at the University of Alberta\n\nCanada in 2006, and he is now a full Professor at the Institute of\n\nPsychology CAS. He has published over 50 papers in major\n\ninternational academic conferences and journals, and the main foci\n\nof his current work are computational cyberpsychology and data\n\nmining.\n\nLocal regression transfer learning with applications to users’ psychological characteristics 153\n\n123\n\n\n\tLocal regression transfer learning with applications to users’ psychological characteristics prediction\n\tAbstract\n\tIntroduction\n\tLocal regression transfer learning\n\tCovariate shift\n\tLocal machine learning\n\tReweighting the importance\n\tK-NN reweighting method\n\tTraining-test K-NN reweighting method\n\tAdaptive K-NN reweighting method\n\tClustering-based reweighting method\n\n\tWeighted regression model\n\n\tExperiments\n\tExperiment setup\n\tPredicting users’ personality across genders\n\tPredicting users’ personality across districts\n\tPredicting users’ depression across genders\n\tDiscussion and conclusion\n\n\tConclusions\n\tAcknowledgments\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9jb3Vyc2VjYXRhbG9nc3RvcmFnZW5yai5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL3M0MDcwOC0wMTUtMDAxNy16LnBkZg2", "metadata_title": "Local regression transfer learning with applications to users’ psychological characteristics prediction", "keyphrases": [ "Local regression transfer learning", "psychological characteristics prediction", "applications" ], "pubication_name": null, "publishers": null, "doi": null, "publication_date": null } ] }